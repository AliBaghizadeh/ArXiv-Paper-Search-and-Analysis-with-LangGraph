{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d45ce147",
   "metadata": {},
   "source": [
    "# Explanation of Key Components\n",
    "\n",
    "\n",
    "## Data Collection Pipeline:       \n",
    "Uses arXiv API to search for papers          \n",
    "Prioritizes HTML versions (available for papers since Dec 2023)            \n",
    "Falls back to abstracts for older papers          \n",
    "Avoids duplicate processing with database checks      \n",
    "\n",
    "## Vector Store Management:         \n",
    "Uses LangChain's Chroma wrapper for compatibility         \n",
    "Stores paper content with rich metadata        \n",
    "Chunks long papers for better retrieval           \n",
    "Enables semantic search with GoogleAI embeddings         \n",
    "\n",
    "## LangGraph RAG Workflow:          \n",
    "Structured workflow with search and generate steps          \n",
    "Maintains state throughout the process        \n",
    "Uses few-shot prompting for high-quality responses         \n",
    "De-duplicates search results            \n",
    "\n",
    "## Enhanced Search Capabilities:         \n",
    "Query expansion with few-shot examples               \n",
    "Deep paper analysis for selected documents          \n",
    "Relevance scoring and ranking          \n",
    "\n",
    "## User Interaction:         \n",
    "Interactive search interface          \n",
    "Options for query expansion             \n",
    "Deep-dive into specific papers           \n",
    "Links to download PDFs             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6a98857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPython version: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:27:10) [MSC v.1938 64 bit (AMD64)]\\nChromaDB version: 0.5.3\\nLangChain version: 0.3.7\\nGoogle GenAI version: 0.8.5\\nlanggraph                 0.3.34                   pypi_0    pypi\\nlanggraph-checkpoint      2.0.24                   pypi_0    pypi\\nlanggraph-prebuilt        0.1.8                    pypi_0    pypi\\nlanggraph-sdk             0.1.63                   pypi_0    pypi\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import arxiv\n",
    "import json\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain & Google GenAI specific\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.tools import Tool\n",
    "import google.generativeai as genai\n",
    "\n",
    "# LangChain vectorstore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# LangGraph specific\n",
    "from typing import TypedDict, Annotated, Sequence, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\"\"\"\n",
    "Python version: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:27:10) [MSC v.1938 64 bit (AMD64)]\n",
    "ChromaDB version: 0.5.3\n",
    "LangChain version: 0.3.7\n",
    "Google GenAI version: 0.8.5\n",
    "langgraph                 0.3.34                   pypi_0    pypi\n",
    "langgraph-checkpoint      2.0.24                   pypi_0    pypi\n",
    "langgraph-prebuilt        0.1.8                    pypi_0    pypi\n",
    "langgraph-sdk             0.1.63                   pypi_0    pypi\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2bdbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U google-generativeai langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "790afb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: models/gemini-1.0-pro-vision-latest\n",
      "Display name: Gemini 1.0 Pro Vision\n",
      "Supported methods: ['generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-pro-vision\n",
      "Display name: Gemini 1.0 Pro Vision\n",
      "Supported methods: ['generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-1.5-pro-latest\n",
      "Display name: Gemini 1.5 Pro Latest\n",
      "Supported methods: ['generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-1.5-pro-001\n",
      "Display name: Gemini 1.5 Pro 001\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-1.5-pro-002\n",
      "Display name: Gemini 1.5 Pro 002\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-1.5-pro\n",
      "Display name: Gemini 1.5 Pro\n",
      "Supported methods: ['generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-1.5-flash-latest\n",
      "Display name: Gemini 1.5 Flash Latest\n",
      "Supported methods: ['generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-1.5-flash-001\n",
      "Display name: Gemini 1.5 Flash 001\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-1.5-flash-001-tuning\n",
      "Display name: Gemini 1.5 Flash 001 Tuning\n",
      "Supported methods: ['generateContent', 'countTokens', 'createTunedModel']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-1.5-flash\n",
      "Display name: Gemini 1.5 Flash\n",
      "Supported methods: ['generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-1.5-flash-002\n",
      "Display name: Gemini 1.5 Flash 002\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-1.5-flash-8b\n",
      "Display name: Gemini 1.5 Flash-8B\n",
      "Supported methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-1.5-flash-8b-001\n",
      "Display name: Gemini 1.5 Flash-8B 001\n",
      "Supported methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-1.5-flash-8b-latest\n",
      "Display name: Gemini 1.5 Flash-8B Latest\n",
      "Supported methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-1.5-flash-8b-exp-0827\n",
      "Display name: Gemini 1.5 Flash 8B Experimental 0827\n",
      "Supported methods: ['generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-1.5-flash-8b-exp-0924\n",
      "Display name: Gemini 1.5 Flash 8B Experimental 0924\n",
      "Supported methods: ['generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.5-pro-exp-03-25\n",
      "Display name: Gemini 2.5 Pro Experimental 03-25\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.5-pro-preview-03-25\n",
      "Display name: Gemini 2.5 Pro Preview 03-25\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.5-flash-preview-04-17\n",
      "Display name: Gemini 2.5 Flash Preview 04-17\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.0-flash-exp\n",
      "Display name: Gemini 2.0 Flash Experimental\n",
      "Supported methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.0-flash\n",
      "Display name: Gemini 2.0 Flash\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.0-flash-001\n",
      "Display name: Gemini 2.0 Flash 001\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.0-flash-lite-001\n",
      "Display name: Gemini 2.0 Flash-Lite 001\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.0-flash-lite\n",
      "Display name: Gemini 2.0 Flash-Lite\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.0-flash-lite-preview-02-05\n",
      "Display name: Gemini 2.0 Flash-Lite Preview 02-05\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.0-flash-lite-preview\n",
      "Display name: Gemini 2.0 Flash-Lite Preview\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.0-pro-exp\n",
      "Display name: Gemini 2.0 Pro Experimental\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.0-pro-exp-02-05\n",
      "Display name: Gemini 2.0 Pro Experimental 02-05\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-exp-1206\n",
      "Display name: Gemini Experimental 1206\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.0-flash-thinking-exp-01-21\n",
      "Display name: Gemini 2.5 Flash Preview 04-17\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.0-flash-thinking-exp\n",
      "Display name: Gemini 2.5 Flash Preview 04-17\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemini-2.0-flash-thinking-exp-1219\n",
      "Display name: Gemini 2.5 Flash Preview 04-17\n",
      "Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "--------------------------------------------------\n",
      "Model name: models/learnlm-1.5-pro-experimental\n",
      "Display name: LearnLM 1.5 Pro Experimental\n",
      "Supported methods: ['generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/learnlm-2.0-flash-experimental\n",
      "Display name: LearnLM 2.0 Flash Experimental\n",
      "Supported methods: ['generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemma-3-1b-it\n",
      "Display name: Gemma 3 1B\n",
      "Supported methods: ['generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemma-3-4b-it\n",
      "Display name: Gemma 3 4B\n",
      "Supported methods: ['generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemma-3-12b-it\n",
      "Display name: Gemma 3 12B\n",
      "Supported methods: ['generateContent', 'countTokens']\n",
      "--------------------------------------------------\n",
      "Model name: models/gemma-3-27b-it\n",
      "Display name: Gemma 3 27B\n",
      "Supported methods: ['generateContent', 'countTokens']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Configure with your API key\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# List all available models\n",
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(f\"Model name: {m.name}\")\n",
    "        print(f\"Display name: {m.display_name}\")\n",
    "        print(f\"Supported methods: {m.supported_generation_methods}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b01e38e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding model: models/embedding-gecko-001\n"
     ]
    }
   ],
   "source": [
    "embedding_models = [m for m in genai.list_models() if \"embedding\" in m.name.lower()]\n",
    "\n",
    "if embedding_models:\n",
    "    # Use the first available embedding model\n",
    "    embedding_model_name = embedding_models[0].name\n",
    "    print(f\"Using embedding model: {embedding_model_name}\")\n",
    "else:\n",
    "    # Fall back to a text model for embeddings\n",
    "    embedding_model_name = \"models/gemini-1.5-pro-latest\"\n",
    "    print(f\"No embedding models found. Using text model for embeddings: {embedding_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a58adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# API Configuration\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\", \"AIzaSyBuYf9Sdm8M8tIMvfArkcS_YUjhEZfZqes\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Initialize Gemini model for RAG workflow\n",
    "gemini_pro = ChatGoogleGenerativeAI(\n",
    "    model=\"models/gemini-1.5-pro-latest\",  # Use exactly this format from the list\n",
    "    temperature=0.3,\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"embedding-gecko-001\",  # Not \"models/embedding-001\"\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90360cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector store with LangChain's wrapper\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"arxiv_papers\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a61f3c",
   "metadata": {},
   "source": [
    "## arXiv Data Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bcad783",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============= arXiv Data Collection Functions =============\n",
    "\n",
    "def arxiv_api_search(query, max_results=3):\n",
    "    \"\"\"Search arXiv for papers matching keywords\"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    return list(search.results())\n",
    "\n",
    "def check_html_availability(paper_id):\n",
    "    \"\"\"Check if HTML version is available for paper\"\"\"\n",
    "    html_url = f\"https://arxiv.org/html/{paper_id}\"\n",
    "    response = requests.head(html_url)\n",
    "    return response.status_code == 200\n",
    "\n",
    "def get_html_content(paper_id):\n",
    "    \"\"\"Get HTML content of paper if available\"\"\"\n",
    "    html_url = f\"https://arxiv.org/html/{paper_id}\"\n",
    "    response = requests.get(html_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Extract main content section\n",
    "        content = soup.find('main')\n",
    "        if content:\n",
    "            # Remove script tags\n",
    "            for script in content.find_all('script'):\n",
    "                script.decompose()\n",
    "            return content.get_text(separator=' ', strip=True)\n",
    "        return \"\"\n",
    "    return None\n",
    "\n",
    "def extract_pdf_abstract(paper):\n",
    "    \"\"\"For papers without HTML, use the abstract as content\"\"\"\n",
    "    return paper.summary\n",
    "\n",
    "def is_paper_in_vectorstore(paper_id):\n",
    "    \"\"\"Check if paper is already in vector store\"\"\"\n",
    "    try:\n",
    "        # Try to retrieve metadata for the paper\n",
    "        results = vectorstore.get(\n",
    "            where={\"paper_id\": paper_id},\n",
    "            limit=1\n",
    "        )\n",
    "        return len(results['ids']) > 0\n",
    "    except:\n",
    "        # If there's an error (e.g., collection doesn't exist), paper is not there\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e26461",
   "metadata": {},
   "source": [
    "## Vector Store Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d9fb895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Vector Store Operations =============\n",
    "\n",
    "def store_paper_in_vector_store(paper, content, content_source):\n",
    "    \"\"\"Store paper and its content in vector store\"\"\"\n",
    "    paper_id = paper.entry_id.split('/')[-1]\n",
    "    \n",
    "    # Chunk content for more effective retrieval\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(content)\n",
    "    \n",
    "    # Prepare metadata\n",
    "    metadatas = [{\n",
    "        \"paper_id\": paper_id,\n",
    "        \"title\": paper.title,\n",
    "        \"authors\": \", \".join(author.name for author in paper.authors),\n",
    "        \"published\": paper.published.strftime(\"%Y-%m-%d\"),\n",
    "        \"url\": paper.entry_id,\n",
    "        \"chunk_id\": i,\n",
    "        \"source\": content_source,\n",
    "        \"abstract\": paper.summary[:500] + \"...\" if len(paper.summary) > 500 else paper.summary\n",
    "    } for i in range(len(chunks))]\n",
    "\n",
    "\n",
    "# Store in vector store\n",
    "    vectorstore.add_texts(\n",
    "        texts=chunks,\n",
    "        metadatas=metadatas,\n",
    "        ids=[f\"{paper_id}_{i}\" for i in range(len(chunks))]\n",
    "    )\n",
    "    \n",
    "    print(f\"Stored {len(chunks)} chunks for paper: {paper.title}\")\n",
    "    return len(chunks)\n",
    "\n",
    "def semantic_search(query, n_results=5):\n",
    "    \"\"\"Search for relevant papers in vector store\"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(\n",
    "        query=query,\n",
    "        k=n_results\n",
    "    )\n",
    "    \n",
    "    formatted_results = []\n",
    "    for doc, score in results:\n",
    "        metadata = doc.metadata\n",
    "        formatted_results.append({\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": metadata,\n",
    "            \"similarity\": 1.0 - float(score)  # Convert distance to similarity\n",
    "        })\n",
    "    \n",
    "    return formatted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df7e3b",
   "metadata": {},
   "source": [
    "## LangGraph RAG Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "001c809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= LangGraph RAG Workflow =============\n",
    "from typing import List, Sequence, Dict, Any, TypedDict\n",
    "\n",
    "# Define the state for our workflow\n",
    "class RAGState(TypedDict):\n",
    "    query: str\n",
    "    context: List[str]\n",
    "    messages: Sequence[HumanMessage | AIMessage | ToolMessage]\n",
    "    search_results: List[Dict[str, Any]]\n",
    "\n",
    "# Search function node\n",
    "def search_papers(state: RAGState) -> RAGState:\n",
    "    \"\"\"Node that performs semantic search\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    results = semantic_search(query, n_results=5)\n",
    "    \n",
    "    context = []\n",
    "    search_results = []\n",
    "    \n",
    "    # Format search results for context\n",
    "    for result in results:\n",
    "        metadata = result[\"metadata\"]\n",
    "        document = result[\"content\"]\n",
    "        \n",
    "        context.append(f\"Paper: {metadata['title']}\\nAuthors: {metadata['authors']}\\nPublished: {metadata['published']}\\nURL: {metadata['url']}\\nAbstract: {metadata['abstract']}\\n\\nExcerpt: {document[:500]}...\\n\\n\")\n",
    "        \n",
    "        # De-duplicate search results by paper_id\n",
    "        if not any(r[\"paper_id\"] == metadata[\"paper_id\"] for r in search_results):\n",
    "            search_results.append({\n",
    "                \"title\": metadata[\"title\"],\n",
    "                \"url\": metadata[\"url\"],\n",
    "                \"paper_id\": metadata[\"paper_id\"],\n",
    "                \"similarity\": result[\"similarity\"]\n",
    "            })\n",
    "    \n",
    "    state[\"context\"] = context\n",
    "    state[\"search_results\"] = search_results\n",
    "    return state\n",
    "\n",
    "# Response generation node\n",
    "def generate_response(state: RAGState) -> RAGState:\n",
    "    \"\"\"Node that generates a response based on context\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    context = state[\"context\"]\n",
    "    \n",
    "    # Format the full prompt with context\n",
    "    few_shot_prompt = \"\"\"\n",
    "    You are a helpful research assistant that provides accurate, relevant information from arXiv papers. I will provide a query and relevant excerpts from papers. Please answer based only on the provided information.\n",
    "\n",
    "    Example Query: \"Is there hybrid convolutional neural networks and vision transformers?\"\n",
    "    Example Context: [A Hybrid Fully Convolutional CNN-Transformer Model for Inherently Interpretable Medical Image Classification]\n",
    "    Example Response: Based on the provided paper, there is an interpretable-by-design hybrid fully convolutional CNN-Transformer architecture for medical image classification. The model enhances interpretability for multi-class tasks.\n",
    "    \n",
    "    Example Query: \"How does the Less-Attention Vision Transformer architecture address the computational inefficiencies and saturation problems of traditional Vision Transformers?\"\n",
    "    Example Context: [You Only Need Less Attention at Each Stage in Vision Transformers]\n",
    "    Example Response: Less-Attention Vision Transformer reduces ViT's quadratic attention cost by reusing early-layer attention scores through linear transformations. It also mitigates attention saturation using residual downsampling and a custom loss to preserve attention structure.\n",
    "    When answering:\n",
    "    1. Only use information present in the provided context\n",
    "    2. Cite specific papers when presenting findings\n",
    "    3. Clearly indicate if the answer is incomplete or if more information is needed\n",
    "    4. Structure your response with clear sections and bullet points\n",
    "    5. Acknowledge limitations in the available research\n",
    "\n",
    "    Now answer my query using only the information in the provided context.\n",
    "\"\"\"\n",
    "    \n",
    "    # Combine context into one string\n",
    "    context_text = \"\\n\".join(context)\n",
    "    \n",
    "    # Create messages\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"{few_shot_prompt}\\n\\nQuery: {query}\\n\\nContext: {context_text}\")\n",
    "    ]\n",
    "    \n",
    "    # Generate response\n",
    "    ai_message = gemini_pro.invoke(messages)\n",
    "    \n",
    "    # Add to state\n",
    "    state[\"messages\"] = list(state.get(\"messages\", [])) + [messages[0], ai_message]\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Build the LangGraph\n",
    "def build_rag_graph():\n",
    "    \"\"\"Create the LangGraph workflow\"\"\"\n",
    "    # Initialize workflow\n",
    "    workflow = StateGraph(RAGState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"search\", search_papers)\n",
    "    workflow.add_node(\"generate\", generate_response)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(\"search\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", END)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"search\")\n",
    "    \n",
    "    # Compile\n",
    "    return workflow.compile()\n",
    "\n",
    "# Create the executable graph\n",
    "rag_graph = build_rag_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276a755",
   "metadata": {},
   "source": [
    "##  Query Expansion with Few-Shot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "780ac5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Query Expansion with Few-Shot =============\n",
    "\n",
    "def expand_research_query(query):\n",
    "    \"\"\"Expand a research query using few-shot prompting\"\"\"\n",
    "    few_shot_examples = \"\"\"\n",
    "    Example 1:\n",
    "    Query: \"transformer models for NLP\"\n",
    "    Expanded: The query is about transformer architecture models used in natural language processing. I should search for papers about BERT, GPT, T5, and other transformer variants, their applications in NLP tasks like translation, summarization, and question answering, and recent improvements to transformer architectures.\n",
    "\n",
    "    Example 2:\n",
    "    Query: \"quantum computing cryptography\"\n",
    "    Expanded: The query relates to the intersection of quantum computing and cryptography. I should search for papers about quantum threats to classical cryptography, post-quantum cryptographic algorithms resistant to quantum attacks, quantum key distribution protocols, and quantum cryptographic primitives.\n",
    "\n",
    "    Example 3:\n",
    "    Query: \"reinforcement learning in robotics\"\n",
    "    Expanded: This query is about applying reinforcement learning techniques to robotic systems. I should search for papers on robotic control using RL, sim-to-real transfer for robotic learning, sample-efficient RL methods for physical systems, deep RL for manipulation tasks, and multi-agent RL for coordinated robots.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Based on the examples below, expand my research query to identify key concepts, relevant subtopics, and specific areas to explore:\n",
    "\n",
    "{few_shot_examples}\n",
    "\n",
    "Query: \"{query}\"\n",
    "Expanded:\"\"\"\n",
    "\n",
    "    response = gemini_pro.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# ============= arXiv Research Pipeline =============\n",
    "\n",
    "def research_arxiv(query, use_expanded_query=True):\n",
    "    \"\"\"Main research function that ties everything together\"\"\"\n",
    "    print(f\"Original query: {query}\")\n",
    "    \n",
    "    # Step 1: Optionally expand the query\n",
    "    if use_expanded_query:\n",
    "        expanded_query = expand_research_query(query)\n",
    "        print(f\"\\nExpanded query:\\n{expanded_query}\")\n",
    "        search_query = f\"{query} {expanded_query}\"\n",
    "    else:\n",
    "        search_query = query\n",
    "    \n",
    "    # Step 2: Get papers from arXiv API\n",
    "    papers = arxiv_api_search(search_query)\n",
    "    print(f\"\\nFound {len(papers)} papers from arXiv API\")\n",
    "    \n",
    "    # Step 3: Process and store papers\n",
    "    papers_processed = 0\n",
    "    for paper in tqdm(papers, desc=\"Processing papers\"):\n",
    "        # Extract paper ID\n",
    "        paper_id = paper.entry_id.split('/')[-1]\n",
    "        \n",
    "        # Check if paper is already in database\n",
    "        if is_paper_in_vectorstore(paper_id):\n",
    "            print(f\"Paper already in database: {paper.title}\")\n",
    "            continue\n",
    "        \n",
    "        # Try to get HTML content first\n",
    "        has_html = check_html_availability(paper_id)\n",
    "        \n",
    "        if has_html:\n",
    "            content = get_html_content(paper_id)\n",
    "            if content:\n",
    "                store_paper_in_vector_store(paper, content, \"html\")\n",
    "                papers_processed += 1\n",
    "                # Add a small delay to avoid overwhelming the server\n",
    "                time.sleep(0.5)\n",
    "                continue\n",
    "        \n",
    "        # Fallback to abstract for older papers\n",
    "        store_paper_in_vector_store(paper, paper.summary, \"abstract\")\n",
    "        papers_processed += 1\n",
    "        # Add a small delay to avoid overwhelming the server\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\nProcessed {papers_processed} new papers\")\n",
    "    \n",
    "    # Step 4: Run the RAG workflow\n",
    "    print(\"\\nGenerating research response...\")\n",
    "    result = rag_graph.invoke({\n",
    "        \"query\": query,\n",
    "        \"context\": [],\n",
    "        \"messages\": [],\n",
    "        \"search_results\": []\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"answer\": result[\"messages\"][-1].content,\n",
    "        \"search_results\": result[\"search_results\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631c88e",
   "metadata": {},
   "source": [
    "## Paper Deep Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97f797b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Paper Deep Analysis =============\n",
    "\n",
    "def analyze_paper_deeply(paper_id, query):\n",
    "    \"\"\"Perform a deeper analysis of a specific paper\"\"\"\n",
    "    # Check if HTML is available\n",
    "    has_html = check_html_availability(paper_id)\n",
    "    \n",
    "    if has_html:\n",
    "        content = get_html_content(paper_id)\n",
    "        source = \"HTML\"\n",
    "    else:\n",
    "        # Get paper from arXiv\n",
    "        search = arxiv.Search(id_list=[paper_id])\n",
    "        results = list(search.results())\n",
    "        if results:\n",
    "            paper = results[0]\n",
    "            content = paper.summary\n",
    "            source = \"Abstract only\"\n",
    "        else:\n",
    "            return \"Paper not found\"\n",
    "    \n",
    "    # Few-shot prompt for paper analysis\n",
    "    analysis_prompt = \"\"\"\n",
    "You are a research assistant performing in-depth analysis of academic papers. Analyze this paper in relation to the research query.\n",
    "\n",
    "Example:\n",
    "Paper: \"Attention Is All You Need\" (Transformer architecture)\n",
    "Query: \"efficient NLP models\"\n",
    "Analysis:\n",
    "# Paper Analysis: Attention Is All You Need\n",
    "\n",
    "## Key Contributions\n",
    "- Introduced the transformer architecture based entirely on self-attention mechanisms\n",
    "- Eliminated recurrence and convolutions entirely in sequence modeling\n",
    "- Achieved state-of-the-art results in machine translation with significantly reduced training time\n",
    "\n",
    "## Methodology\n",
    "- Self-attention mechanism computes representation of a sequence by relating different positions\n",
    "- Multi-head attention allows the model to jointly attend to information from different representation subspaces\n",
    "- Position-wise feed-forward networks apply the same feed-forward network to each position\n",
    "\n",
    "## Notable Findings\n",
    "- Achieved BLEU score of 28.4 on English-to-German translation, outperforming previous best models\n",
    "- Training was 3.5x faster than the best previous models on the same hardware\n",
    "- Demonstrated effectiveness on English constituency parsing without any task-specific tuning\n",
    "\n",
    "## Relevance to Query\n",
    "- Directly addresses efficiency by significantly reducing training time\n",
    "- The parallelizable nature of attention enables much faster training on modern hardware\n",
    "- The architecture has become the foundation for many efficient NLP models that followed\n",
    "\n",
    "## Limitations\n",
    "- High memory requirements for very long sequences (quadratic complexity)\n",
    "- Position encoding scheme may not be optimal for all sequence modeling tasks\n",
    "- Limited evaluation on tasks beyond machine translation\n",
    "\n",
    "Now analyze the following paper:\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"{analysis_prompt}\\n\\nPaper content ({source}):\\n{content[:5000]}\\n\\nQuery: {query}\\n\\nAnalysis:\"\n",
    "    \n",
    "    response = gemini_pro.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9276d6",
   "metadata": {},
   "source": [
    "## User Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55600a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= User Interaction =============\n",
    "\n",
    "def run_interactive_search():\n",
    "    \"\"\"Interactive search function\"\"\"\n",
    "    query = input(\"Enter your research query: \")\n",
    "    \n",
    "    # Ask if user wants to expand the query\n",
    "    expand = input(\"Do you want to expand the query with few-shot prompting? (y/n, default=y): \").lower() != 'n'\n",
    "    \n",
    "    print(\"\\nSearching and analyzing papers...\")\n",
    "    results = research_arxiv(query, use_expanded_query=expand)\n",
    "    \n",
    "    print(\"\\n=== RESEARCH RESULTS ===\\n\")\n",
    "    display(Markdown(results[\"answer\"]))\n",
    "    \n",
    "    print(\"\\n=== TOP PAPERS ===\\n\")\n",
    "    for i, paper in enumerate(results[\"search_results\"][:5]):\n",
    "        print(f\"{i+1}. {paper['title']}\")\n",
    "        print(f\"   URL: {paper['url']}\")\n",
    "        print(f\"   Relevance: {paper['similarity']:.2f}\\n\")\n",
    "    \n",
    "    # Ask if user wants to explore any paper in depth\n",
    "    while True:\n",
    "        selection = input(\"\\nEnter paper number for deeper analysis (or 0 to exit): \")\n",
    "        if selection == '0' or not selection.strip():\n",
    "            break\n",
    "            \n",
    "        if selection.isdigit() and 1 <= int(selection) <= len(results[\"search_results\"]):\n",
    "            paper_id = results[\"search_results\"][int(selection)-1][\"paper_id\"]\n",
    "            paper_url = results[\"search_results\"][int(selection)-1][\"url\"]\n",
    "            paper_title = results[\"search_results\"][int(selection)-1][\"title\"]\n",
    "            \n",
    "            print(f\"\\nPerforming deep analysis of paper: {paper_title}...\")\n",
    "            analysis = analyze_paper_deeply(paper_id, query)\n",
    "            \n",
    "            print(\"\\n=== PAPER ANALYSIS ===\\n\")\n",
    "            display(Markdown(analysis))\n",
    "            \n",
    "            print(f\"\\nPaper URL: {paper_url}\")\n",
    "            \n",
    "            # Option to download PDF\n",
    "            download = input(\"Do you want to download the PDF? (y/n): \").lower()\n",
    "            if download == 'y':\n",
    "                print(f\"PDF available at: https://arxiv.org/pdf/{paper_id}.pdf\")\n",
    "        else:\n",
    "            print(\"Invalid selection. Please try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a436f88",
   "metadata": {},
   "source": [
    "##  Example Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2c3d241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching and analyzing papers...\n",
      "Original query: is there hybrid cnn and vision transformers?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18056\\340839207.py:11: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  return list(search.results())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 3 papers from arXiv API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:   0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content: 400 * BatchEmbedContentsRequest.model: unexpected model name format\n* BatchEmbedContentsRequest.requests[0].model: unexpected model name format\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:228\u001b[0m, in \u001b[0;36membed_documents\u001b[1;34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mbatch_embed_contents(\n\u001b[1;32m--> 228\u001b[0m         BatchEmbedContentsRequest(requests\u001b[38;5;241m=\u001b[39mrequests, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    229\u001b[0m     )\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:1436\u001b[0m, in \u001b[0;36mbatch_embed_contents\u001b[1;34m(self, request, model, requests, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_tokens\u001b[39m(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     request: Optional[Union[generative_service\u001b[38;5;241m.\u001b[39mCountTokensRequest, \u001b[38;5;28mdict\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1410\u001b[0m     metadata: Sequence[Tuple[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m]]] \u001b[38;5;241m=\u001b[39m (),\n\u001b[0;32m   1411\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m generative_service\u001b[38;5;241m.\u001b[39mCountTokensResponse:\n\u001b[0;32m   1412\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a model's tokenizer on input ``Content`` and returns the\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;124;03m    token count. Refer to the `tokens\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;124;03m    guide <https://ai.google.dev/gemini-api/docs/tokens>`__ to learn\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;124;03m    more about tokens.\u001b[39;00m\n\u001b[0;32m   1416\u001b[0m \n\u001b[0;32m   1417\u001b[0m \u001b[38;5;124;03m    .. code-block:: python\u001b[39;00m\n\u001b[0;32m   1418\u001b[0m \n\u001b[0;32m   1419\u001b[0m \u001b[38;5;124;03m        # This snippet has been automatically generated and should be regarded as a\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;124;03m        # code template only.\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;124;03m        # It will require modifications to work:\u001b[39;00m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;124;03m        # - It may require correct/in-range values for request initialization.\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;124;03m        # - It may require specifying regional endpoints when creating the service\u001b[39;00m\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;124;03m        #   client as shown in:\u001b[39;00m\n\u001b[0;32m   1425\u001b[0m \u001b[38;5;124;03m        #   https://googleapis.dev/python/google-api-core/latest/client_options.html\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;124;03m        from google.ai import generativelanguage_v1beta\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m \n\u001b[0;32m   1428\u001b[0m \u001b[38;5;124;03m        def sample_count_tokens():\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;124;03m            # Create a client\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;124;03m            client = generativelanguage_v1beta.GenerativeServiceClient()\u001b[39;00m\n\u001b[0;32m   1431\u001b[0m \n\u001b[0;32m   1432\u001b[0m \u001b[38;5;124;03m            # Initialize request argument(s)\u001b[39;00m\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;124;03m            request = generativelanguage_v1beta.CountTokensRequest(\u001b[39;00m\n\u001b[0;32m   1434\u001b[0m \u001b[38;5;124;03m                model=\"model_value\",\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[1;32m-> 1436\u001b[0m \n\u001b[0;32m   1437\u001b[0m \u001b[38;5;124;03m            # Make the request\u001b[39;00m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;124;03m            response = client.count_tokens(request=request)\u001b[39;00m\n\u001b[0;32m   1439\u001b[0m \n\u001b[0;32m   1440\u001b[0m \u001b[38;5;124;03m            # Handle the response\u001b[39;00m\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;124;03m            print(response)\u001b[39;00m\n\u001b[0;32m   1442\u001b[0m \n\u001b[0;32m   1443\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;124;03m        request (Union[google.ai.generativelanguage_v1beta.types.CountTokensRequest, dict]):\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;124;03m            The request object. Counts the number of tokens in the ``prompt`` sent to a\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03m            model.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[0;32m   1448\u001b[0m \u001b[38;5;124;03m            Models may tokenize text differently, so each model may\u001b[39;00m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;124;03m            return a different ``token_count``.\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;124;03m        model (str):\u001b[39;00m\n\u001b[0;32m   1451\u001b[0m \u001b[38;5;124;03m            Required. The model's resource name. This serves as an\u001b[39;00m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;124;03m            ID for the Model to use.\u001b[39;00m\n\u001b[0;32m   1453\u001b[0m \n\u001b[0;32m   1454\u001b[0m \u001b[38;5;124;03m            This name should match a model name returned by the\u001b[39;00m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;124;03m            ``ListModels`` method.\u001b[39;00m\n\u001b[0;32m   1456\u001b[0m \n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m            Format: ``models/{model}``\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m \n\u001b[0;32m   1459\u001b[0m \u001b[38;5;124;03m            This corresponds to the ``model`` field\u001b[39;00m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;124;03m            on the ``request`` instance; if ``request`` is provided, this\u001b[39;00m\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;124;03m            should not be set.\u001b[39;00m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;124;03m        contents (MutableSequence[google.ai.generativelanguage_v1beta.types.Content]):\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;124;03m            Optional. The input given to the model as a prompt. This\u001b[39;00m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;124;03m            field is ignored when ``generate_content_request`` is\u001b[39;00m\n\u001b[0;32m   1465\u001b[0m \u001b[38;5;124;03m            set.\u001b[39;00m\n\u001b[0;32m   1466\u001b[0m \n\u001b[0;32m   1467\u001b[0m \u001b[38;5;124;03m            This corresponds to the ``contents`` field\u001b[39;00m\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;124;03m            on the ``request`` instance; if ``request`` is provided, this\u001b[39;00m\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;124;03m            should not be set.\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;124;03m        retry (google.api_core.retry.Retry): Designation of what errors, if any,\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;124;03m            should be retried.\u001b[39;00m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;124;03m        timeout (float): The timeout for this request.\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;124;03m        metadata (Sequence[Tuple[str, Union[str, bytes]]]): Key/value pairs which should be\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;124;03m            sent along with the request as metadata. Normally, each value must be of type `str`,\u001b[39;00m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;124;03m            but for metadata keys ending with the suffix `-bin`, the corresponding values must\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;124;03m            be of type `bytes`.\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \n\u001b[0;32m   1478\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;124;03m        google.ai.generativelanguage_v1beta.types.CountTokensResponse:\u001b[39;00m\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;124;03m            A response from CountTokens.\u001b[39;00m\n\u001b[0;32m   1481\u001b[0m \n\u001b[0;32m   1482\u001b[0m \u001b[38;5;124;03m               It returns the model's token_count for the prompt.\u001b[39;00m\n\u001b[0;32m   1483\u001b[0m \n\u001b[0;32m   1484\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1485\u001b[0m     \u001b[38;5;66;03m# Create or coerce a protobuf request object.\u001b[39;00m\n\u001b[0;32m   1486\u001b[0m     \u001b[38;5;66;03m# - Quick check: If we got a request object, we should *not* have\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m     \u001b[38;5;66;03m#   gotten any keyword arguments that map to the request.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgument\u001b[0m: 400 * BatchEmbedContentsRequest.model: unexpected model name format\n* BatchEmbedContentsRequest.requests[0].model: unexpected model name format\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGoogleGenerativeAIError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Interactive mode\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mrun_interactive_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Or direct usage:\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# results = research_arxiv(\"transformer models for efficient NLP\")\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# print(results[\"answer\"])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 11\u001b[0m, in \u001b[0;36mrun_interactive_search\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m expand \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo you want to expand the query with few-shot prompting? (y/n, default=y): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSearching and analyzing papers...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mresearch_arxiv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_expanded_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== RESEARCH RESULTS ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m display(Markdown(results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[49], line 71\u001b[0m, in \u001b[0;36mresearch_arxiv\u001b[1;34m(query, use_expanded_query)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Fallback to abstract for older papers\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[43mstore_paper_in_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m papers_processed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Add a small delay to avoid overwhelming the server\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[47], line 29\u001b[0m, in \u001b[0;36mstore_paper_in_vector_store\u001b[1;34m(paper, content, content_source)\u001b[0m\n\u001b[0;32m     16\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [{\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaper_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: paper_id,\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: paper\u001b[38;5;241m.\u001b[39mtitle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m\"\u001b[39m: paper\u001b[38;5;241m.\u001b[39msummary[:\u001b[38;5;241m500\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(paper\u001b[38;5;241m.\u001b[39msummary) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m paper\u001b[38;5;241m.\u001b[39msummary\n\u001b[0;32m     25\u001b[0m     } \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chunks))]\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Store in vector store\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     \u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpaper_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStored \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks for paper: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper\u001b[38;5;241m.\u001b[39mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunks)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:232\u001b[0m, in \u001b[0;36membed_documents\u001b[1;34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GoogleGenerativeAIError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError embedding content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;28mlist\u001b[39m(e\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39membeddings])\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[1;31mGoogleGenerativeAIError\u001b[0m: Error embedding content: 400 * BatchEmbedContentsRequest.model: unexpected model name format\n* BatchEmbedContentsRequest.requests[0].model: unexpected model name format\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Interactive mode\n",
    "    run_interactive_search()\n",
    "    \n",
    "    # Or direct usage:\n",
    "    # results = research_arxiv(\"transformer models for efficient NLP\")\n",
    "    # print(results[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eabbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
