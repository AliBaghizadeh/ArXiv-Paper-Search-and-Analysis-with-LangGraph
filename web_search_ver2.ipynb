{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d45ce147",
   "metadata": {},
   "source": [
    "# Explanation of Key Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee34ba7c",
   "metadata": {},
   "source": [
    "**This version does not use Embedings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262baab2",
   "metadata": {},
   "source": [
    "\n",
    "### Data Collection Pipeline:\n",
    "\n",
    "Uses arXiv API to search for papers based on research queries               \n",
    "Prioritizes HTML versions of papers (available for papers since Dec 2023)               \n",
    "Falls back to abstracts for older papers without HTML versions          \n",
    "Caches paper information to avoid redundant processing            \n",
    "Extracts clean text content using BeautifulSoup for HTML papers     \n",
    "\n",
    "### Query Enhancement:\n",
    "\n",
    "Employs few-shot prompting with domain-specific examples        \n",
    "Expands original queries to include related concepts and terminology         \n",
    "Extracts key terms from expanded queries for more effective searching         \n",
    "Adapts to specific research domains with customized examples          \n",
    "Handles both general research and specialized topics (like Vision Transformers)    \n",
    "\n",
    "### LangGraph RAG Workflow:\n",
    "\n",
    "Implements a structured workflow with defined nodes and transitions       \n",
    "Maintains comprehensive state throughout the research process          \n",
    "Four-stage pipeline: query expansion → paper search → analysis → response generation         \n",
    "Each node updates specific parts of the state without losing information          \n",
    "Handles the full research journey from question to comprehensive analysis      \n",
    "\n",
    "### Paper Analysis Capabilities:\n",
    "\n",
    "Generates in-depth analyses of retrieved papers using few-shot learning       \n",
    "Identifies connections between papers and research question           \n",
    "Extracts key contributions, methodologies, and technical details           \n",
    "Provides research context through carefully selected examples          \n",
    "Synthesizes information across multiple papers for comprehensive understanding          \n",
    "\n",
    "### User Interaction:\n",
    "\n",
    "Provides formatted Markdown output for easy reading in notebooks      \n",
    "Displays expanded query to show understanding of research needs         \n",
    "Presents comprehensive research analysis with insights and connections          \n",
    "Lists top papers with titles, authors, publication dates, and abstracts       \n",
    "Includes direct links to original papers on arXiv          \n",
    "Simple interface for entering research queries and viewing results          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c17ae",
   "metadata": {},
   "source": [
    "## Customizable Parameters in the Pipeline\n",
    "\n",
    "Here are the key parameters to modify to experiment with results:\n",
    "\n",
    "**Model Parameters**     \n",
    "Model Version: Change model = genai.GenerativeModel('models/gemini-1.5-pro-latest') to use a different Gemini model         \n",
    "Temperature: Add temperature parameter when creating the model to control creativity (e.g., model = genai.GenerativeModel('models/gemini-1.5-pro-latest', temperature=0.2))             \n",
    "\n",
    "**Search Parameters**\n",
    "Max Results: Modify max_results=20 in arxiv_api_search() to return more/fewer papers                  \n",
    "Sort Criteria: Change sort_by=arxiv.SortCriterion.Relevance to sort by other criteria like Submitted or LastUpdated             \n",
    "Category Filter: Customize the category filter logic in arxiv_api_search() (currently set to cs.CV for vision-related queries)              \n",
    "\n",
    "**Content Processing**\n",
    "Papers Analyzed: Change the number of papers analyzed in analyze_papers_node() and analyze_papers() (currently uses top 5)             \n",
    "Few-Shot Examples: Modify the examples in expand_research_query() and analyze_papers() to better fit your domain         \n",
    "\n",
    "**Display Settings**\n",
    "Display Limit: Change the slice in results[\"search_results\"][:5] in display_langgraph_results() to show more papers         \n",
    "Abstract Length: Adjust the paper['abstract'][:300] to show more/less text      \n",
    "\n",
    "**Workflow Configuration**\n",
    "Node Ordering: Rearrange the workflow by modifying edges in create_research_workflow()               \n",
    "Initial State: Add additional fields to the initial state in research_arxiv_langgraph()            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2bdbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U google-generativeai langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac61639",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c0da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Imports and Configuration =============\n",
    "\n",
    "import os\n",
    "import arxiv\n",
    "import requests\n",
    "import numpy as np\n",
    "from typing import TypedDict, List, Dict, Any, Sequence\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# LLM and message handling imports\n",
    "import google.generativeai as genai\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "\"\"\"\n",
    "langgraph  Version: 0.3.34\n",
    "google-generativeai Version: 0.8.5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949084a0",
   "metadata": {},
   "source": [
    "## State and Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b3d0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGState(TypedDict):\n",
    "    \"\"\"State for the RAG workflow.\n",
    "    \n",
    "    Attributes:\n",
    "        query: The original user query\n",
    "        expanded_query: Query after expansion with the LLM\n",
    "        context: List of contextualized information from papers\n",
    "        messages: List of chat messages in the conversation\n",
    "        search_results: List of papers retrieved from arXiv\n",
    "        analysis: Generated analysis of the papers\n",
    "    \"\"\"\n",
    "    query: str\n",
    "    expanded_query: str\n",
    "    context: List[str]\n",
    "    messages: List[Dict[str, Any]]\n",
    "    search_results: List[Dict[str, Any]]\n",
    "    analysis: str\n",
    "\n",
    "\n",
    "# API Configuration\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\", \"AIzaSyBuYf9Sdm8M8tIMvfArkcS_YUjhEZfZqes\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Create a model instance\n",
    "model = genai.GenerativeModel('models/gemini-1.5-pro-latest')\n",
    "\n",
    "# Papers database cache\n",
    "papers_db = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe80951",
   "metadata": {},
   "source": [
    "## Query Expansion and Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1fcc6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_research_query(query: str) -> str:\n",
    "    \"\"\"Expand a research query using few-shot prompting.\n",
    "    \n",
    "    Uses domain-specific examples to help the model generate\n",
    "    a comprehensive expansion of the original query.\n",
    "    \n",
    "    Args:\n",
    "        query: The original research query\n",
    "        \n",
    "    Returns:\n",
    "        An expanded version of the query with additional concepts and terms\n",
    "    \"\"\"\n",
    "    # Vision transformer specific example if the query is about vision transformers\n",
    "    if \"vision transformer\" in query.lower() or \"vit\" in query.lower():\n",
    "        few_shot_examples = \"\"\"\n",
    "        Example 1:\n",
    "        Query: \"vision transformer architecture\"\n",
    "        Expanded: The query is about Vision Transformer (ViT) architectures for computer vision tasks. Key aspects to explore include: original ViT design and patch-based image tokenization; comparison with CNN architectures; attention mechanisms specialized for vision; hierarchical and pyramid vision transformers; efficiency improvements like token pruning and sparse attention; distillation techniques for vision transformers; adaptations for different vision tasks including detection and segmentation; recent innovations addressing quadratic complexity and attention saturation.\n",
    "        \n",
    "        Example 2: \n",
    "        Query: \"how do vision transformers process images\"\n",
    "        Expanded: The query focuses on the internal mechanisms of how Vision Transformers process visual information. Key areas to investigate include: patch embedding processes; position embeddings for spatial awareness; self-attention mechanisms for global context; the role of MLP blocks in feature transformation; how class tokens aggregate information; patch size impact on performance and efficiency; multi-head attention design in vision applications; information flow through vision transformer layers; differences from convolutional approaches to feature extraction.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        few_shot_examples = \"\"\"\n",
    "        Example 1:\n",
    "        Query: \"transformer models for NLP\"\n",
    "        Expanded: The query is about transformer architecture models used in natural language processing. Key aspects to explore include: BERT, GPT, T5, and other transformer variants; attention mechanisms; self-supervision and pre-training approaches; fine-tuning methods; performance on NLP tasks like translation, summarization, and question answering; efficiency improvements like distillation and pruning; recent innovations in transformer architectures.\n",
    "        \n",
    "        Example 2:\n",
    "        Query: \"reinforcement learning for robotics\"\n",
    "        Expanded: The query concerns applying reinforcement learning methods to robotic systems. Important areas to investigate include: policy gradient methods; Q-learning variants for continuous control; sim-to-real transfer; imitation learning; model-based RL for robotics; sample efficiency techniques; multi-agent RL for coordinated robots; safety constraints in robotic RL; real-world applications and benchmarks; hierarchical RL for complex tasks.\n",
    "        \n",
    "        Example 3:\n",
    "        Query: \"graph neural networks applications\"\n",
    "        Expanded: The query focuses on practical applications of graph neural networks. Key dimensions to explore include: GNN architectures (GCN, GAT, GraphSAGE); applications in chemistry and drug discovery; recommender systems using GNNs; traffic and transportation network modeling; social network analysis; knowledge graph completion; GNNs for computer vision tasks; scalability solutions for large graphs; theoretical foundations of graph representation learning.\n",
    "        \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Based on the examples below, expand my research query to identify key concepts, relevant subtopics, and specific areas to explore:\n",
    "\n",
    "    {few_shot_examples}\n",
    "\n",
    "    Query: \"{query}\"\n",
    "    Expanded:\"\"\"\n",
    "    \n",
    "    generation_config = {\"temperature\": 1.0}\n",
    "    \n",
    "    response = model.generate_content(prompt, generation_config=generation_config)\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "\n",
    "def analyze_papers(query: str, papers: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Analyze papers using few-shot prompting with domain-specific examples.\n",
    "    \n",
    "    Generates a research analysis based on the retrieved papers and\n",
    "    the original query using domain-specific examples.\n",
    "    \n",
    "    Args:\n",
    "        query: The original research query\n",
    "        papers: List of paper dictionaries containing metadata and content\n",
    "        \n",
    "    Returns:\n",
    "        A comprehensive analysis of the papers in relation to the query\n",
    "    \"\"\"\n",
    "    few_shot_examples = \"\"\"\n",
    "    Example 1:\n",
    "    Papers:\n",
    "    1. \"Attention Is All You Need\" - Introduced the transformer architecture relying entirely on attention mechanisms without recurrence or convolutions.\n",
    "    2. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" - Proposed bidirectional training for transformers using masked language modeling.\n",
    "    \n",
    "    Analysis:\n",
    "    These papers represent seminal work in transformer architectures for NLP. \"Attention Is All You Need\" established the foundation with the original transformer design using multi-head self-attention. BERT built upon this by introducing bidirectional context modeling and masked language modeling for pre-training, significantly advancing performance on downstream tasks. Key themes include attention mechanisms, pre-training objectives, and the importance of training methodology.\n",
    "    \n",
    "    Example 2:\n",
    "    Query: \"How does the Less-Attention Vision Transformer architecture address the computational inefficiencies and saturation problems of traditional Vision Transformers?\"\n",
    "    Papers: \n",
    "    1. \"You Only Need Less Attention at Each Stage in Vision Transformers\" - Proposed reusing early-layer attention scores through linear transformations to reduce computational costs.\n",
    "    \n",
    "    Analysis:\n",
    "    Less-Attention Vision Transformer reduces ViT's quadratic attention cost by reusing early-layer attention scores through linear transformations. It also mitigates attention saturation using residual downsampling and a custom loss to preserve attention structure. This approach addresses two key limitations of traditional Vision Transformers: computational inefficiency due to quadratic complexity of self-attention, and the saturation problem where attention maps become increasingly similar in deeper layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format paper information\n",
    "    paper_info = \"\\n\".join([\n",
    "        f\"{i+1}. \\\"{p['title']}\\\" - {p['abstract'][:200]}...\" \n",
    "        for i, p in enumerate(papers[:5])\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"Based on the examples below, analyze the following research papers related to \"{query}\" to identify key technical contributions, methodologies, and how they address specific challenges:\n",
    "\n",
    "    {few_shot_examples}\n",
    "    \n",
    "    Papers:\n",
    "    {paper_info}\n",
    "    \n",
    "    Analysis:\"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt, generation_config={\"temperature\": 0.7})\n",
    "    \n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812cc982",
   "metadata": {},
   "source": [
    "## arXiv Data Collection Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64cf7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_api_search(query: str, max_results: int = 20) -> List[Any]:\n",
    "    \"\"\"Search arXiv for papers matching the query.\n",
    "    \n",
    "    Uses the arXiv API to find relevant papers based on the query,\n",
    "    with optional filtering for specific categories.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        max_results: Maximum number of results to return (default: 20)\n",
    "        \n",
    "    Returns:\n",
    "        List of arxiv.Result objects representing papers\n",
    "    \"\"\"\n",
    "    # Use category filter for more relevant results\n",
    "    category_filter = \"cat:cs.CV\" if \"vision\" in query.lower() or \"image\" in query.lower() else \"\"\n",
    "    search_query = f\"{query} {category_filter}\".strip()\n",
    "    \n",
    "    # Create a Client instance and use it for search\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query=search_query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    return list(client.results(search))\n",
    "\n",
    "\n",
    "def check_html_available(paper_id: str) -> bool:\n",
    "    \"\"\"Check if HTML version is available for a paper.\n",
    "    \n",
    "    Tests if the paper has an HTML version on arXiv by\n",
    "    sending a HEAD request to the HTML URL.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: The arXiv paper ID\n",
    "        \n",
    "    Returns:\n",
    "        True if HTML version is available, False otherwise\n",
    "    \"\"\"\n",
    "    html_url = f\"https://arxiv.org/html/{paper_id}\"\n",
    "    response = requests.head(html_url)\n",
    "    return response.status_code == 200\n",
    "\n",
    "\n",
    "def get_html_content(paper_id: str) -> str:\n",
    "    \"\"\"Get HTML content of a paper if available.\n",
    "    \n",
    "    Fetches and parses the HTML version of a paper, removing\n",
    "    irrelevant elements and extracting the main content.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: The arXiv paper ID\n",
    "        \n",
    "    Returns:\n",
    "        Extracted text content from the HTML version or None if unavailable\n",
    "    \"\"\"\n",
    "    html_url = f\"https://arxiv.org/html/{paper_id}\"\n",
    "    response = requests.get(html_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Remove scripts, styles, and navigation elements\n",
    "        for tag in soup(['script', 'style', 'nav', 'header', 'footer']):\n",
    "            tag.decompose()\n",
    "        # Get main content\n",
    "        main_content = soup.find('main') or soup.find('body')\n",
    "        if main_content:\n",
    "            return main_content.get_text(separator='\\n', strip=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84063ed7",
   "metadata": {},
   "source": [
    "## LangGraph Workflow Nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2b6cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_expansion_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"LangGraph node that expands the original query.\n",
    "    \n",
    "    Takes the original query and generates an expanded version\n",
    "    with additional concepts and search terms.\n",
    "    \n",
    "    Args:\n",
    "        state: The current LangGraph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with expanded query\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    # Use a very explicit prompt to avoid the model repeating our instructions\n",
    "    prompt = f\"\"\"\n",
    "    Please expand the following research query about vision transformers:\n",
    "    \n",
    "    Query: \"{query}\"\n",
    "    \n",
    "    Provide a detailed expansion that identifies key concepts, \n",
    "    terminology, and relevant subtopics. Do not include phrases like\n",
    "    \"Query:\" or \"Expanded:\" in your response. Just provide the expanded content.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    expanded_query = response.text.strip()\n",
    "    \n",
    "    print(\"EXPANSION NODE - Output expanded query:\", expanded_query[:100] + \"...\")\n",
    "    \n",
    "    return {\"expanded_query\": expanded_query}\n",
    "\n",
    "\n",
    "def search_papers_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"LangGraph node that searches for papers based on the query.\n",
    "    \n",
    "    Uses the original and expanded query to search arXiv for relevant\n",
    "    papers, processes them, and stores the results.\n",
    "    \n",
    "    Args:\n",
    "        state: The current LangGraph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with search results\n",
    "    \"\"\"\n",
    "    print(\"SEARCH NODE - Input state keys:\", list(state.keys()))\n",
    "    \n",
    "    query = state[\"query\"]\n",
    "    expanded_query = state[\"expanded_query\"]\n",
    "    \n",
    "    # Extract actual search terms from expanded query\n",
    "    # We need to clean up the expanded query to just get the keywords\n",
    "    # Instead of using the full text, let's extract key concepts\n",
    "    \n",
    "    # First, check if expanded query starts with \"Query:\" (which would indicate our formatting issue)\n",
    "    if \"Query:\" in expanded_query and \"Expanded:\" in expanded_query:\n",
    "        # Extract just the expansion part\n",
    "        expanded_query = expanded_query.split(\"Expanded:\")[1].strip()\n",
    "    \n",
    "    # Now extract key concepts - focus on noun phrases\n",
    "    key_terms = []\n",
    "    important_phrases = [\"Vision Transformer\", \"ViT\", \"image patches\", \n",
    "                        \"self-attention\", \"transformer encoder\", \n",
    "                        \"multi-head attention\", \"computer vision\"]\n",
    "    \n",
    "    # Add any terms from our list that appear in the expanded query\n",
    "    for phrase in important_phrases:\n",
    "        if phrase.lower() in expanded_query.lower():\n",
    "            key_terms.append(phrase)\n",
    "    \n",
    "    # Ensure we have the core terms at minimum\n",
    "    if not any(term for term in key_terms if \"vision transformer\" in term.lower() or \"vit\" in term.lower()):\n",
    "        key_terms.append(\"Vision Transformer\")\n",
    "    \n",
    "    # Join terms with OR\n",
    "    expanded_terms = \" OR \".join(f'\"{term}\"' for term in key_terms)\n",
    "    \n",
    "    # Create a clean search query\n",
    "    search_query = f'\"{query}\" OR ({expanded_terms})'\n",
    "    print(f\"Clean search query: {search_query}\")\n",
    "    \n",
    "    papers = arxiv_api_search(search_query)\n",
    "    \n",
    "    print(f\"Found {len(papers)} papers\")\n",
    "    \n",
    "    # Process and store papers\n",
    "    results = []\n",
    "    for paper in papers:\n",
    "        paper_id = paper.entry_id.split('/')[-1]\n",
    "        paper_info = {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"title\": paper.title,\n",
    "            \"authors\": \", \".join(author.name for author in paper.authors),\n",
    "            \"published\": paper.published.strftime(\"%Y-%m-%d\"),\n",
    "            \"url\": paper.entry_id,\n",
    "            \"abstract\": paper.summary,\n",
    "            \"has_html\": check_html_available(paper_id)\n",
    "        }\n",
    "        \n",
    "        # Get HTML content if available\n",
    "        if paper_info[\"has_html\"]:\n",
    "            paper_info[\"content\"] = get_html_content(paper_id)\n",
    "        else:\n",
    "            paper_info[\"content\"] = paper_info[\"abstract\"]\n",
    "            \n",
    "        papers_db[paper_id] = paper_info\n",
    "        results.append(paper_info)\n",
    "    \n",
    "    print(f\"Processed {len(results)} papers for state\")\n",
    "    \n",
    "    # Return updated state\n",
    "    updated_state = {\"search_results\": results}\n",
    "    print(\"SEARCH NODE - Output state keys:\", list(updated_state.keys()))\n",
    "    return updated_state\n",
    "\n",
    "\n",
    "def analyze_papers_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"LangGraph node that analyzes papers.\n",
    "    \n",
    "    Takes the search results and generates an analysis\n",
    "    of the papers in relation to the original query.\n",
    "    \n",
    "    Args:\n",
    "        state: The current LangGraph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with paper analysis and context\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    search_results = state[\"search_results\"]\n",
    "    \n",
    "    analysis = analyze_papers(query, search_results)\n",
    "    \n",
    "    # Extract relevant content for context\n",
    "    context = []\n",
    "    for paper in search_results[:5]:\n",
    "        context.append(f\"Title: {paper['title']}\\nAuthors: {paper['authors']}\\nAbstract: {paper['abstract']}\")\n",
    "    \n",
    "    return {\n",
    "        \"analysis\": analysis,\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_response_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"LangGraph node that generates the final response.\n",
    "    \n",
    "    Formats the analysis and adds it to the message history\n",
    "    in the state.\n",
    "    \n",
    "    Args:\n",
    "        state: The current LangGraph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with messages\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    context = state[\"context\"]\n",
    "    analysis = state[\"analysis\"]\n",
    "    \n",
    "    # Format the message as an AI response\n",
    "    message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": analysis\n",
    "    }\n",
    "    \n",
    "    # Add the message to the state\n",
    "    if \"messages\" not in state:\n",
    "        state[\"messages\"] = []\n",
    "    \n",
    "    state[\"messages\"].append(message)\n",
    "    \n",
    "    return {\"messages\": state[\"messages\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ffc61f",
   "metadata": {},
   "source": [
    "## LangGraph Workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7bf47f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_research_workflow():\n",
    "    \"\"\"Create a LangGraph workflow for research.\n",
    "    \n",
    "    Defines the workflow graph with nodes for query expansion,\n",
    "    paper search, analysis, and response generation.\n",
    "    \n",
    "    Returns:\n",
    "        A compiled LangGraph workflow\n",
    "    \"\"\"\n",
    "    # Initialize the workflow with the RAGState\n",
    "    workflow = StateGraph(RAGState)\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    workflow.add_node(\"query_expansion\", query_expansion_node)\n",
    "    workflow.add_node(\"search_papers\", search_papers_node)\n",
    "    workflow.add_node(\"analyze_papers\", analyze_papers_node)\n",
    "    workflow.add_node(\"generate_response\", generate_response_node)\n",
    "    \n",
    "    # Add edges to connect the nodes\n",
    "    workflow.add_edge(\"query_expansion\", \"search_papers\")\n",
    "    workflow.add_edge(\"search_papers\", \"analyze_papers\")\n",
    "    workflow.add_edge(\"analyze_papers\", \"generate_response\")\n",
    "    \n",
    "    # Set the entry point\n",
    "    workflow.set_entry_point(\"query_expansion\")\n",
    "    \n",
    "    # Compile the workflow\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c51291",
   "metadata": {},
   "source": [
    "## Interface Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63a5ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_arxiv_langgraph(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Research arXiv papers using the LangGraph workflow.\n",
    "    \n",
    "    Main function that executes the full research pipeline\n",
    "    on a given query.\n",
    "    \n",
    "    Args:\n",
    "        query: The research query\n",
    "        \n",
    "    Returns:\n",
    "        The final state with all results\n",
    "    \"\"\"\n",
    "    # Create the workflow\n",
    "    workflow = create_research_workflow()\n",
    "    \n",
    "    # Initialize the state\n",
    "    initial_state = {\n",
    "        \"query\": query,\n",
    "        \"expanded_query\": \"\",\n",
    "        \"context\": [],\n",
    "        \"messages\": [],\n",
    "        \"search_results\": [],\n",
    "        \"analysis\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Execute the workflow\n",
    "    final_state = workflow.invoke(initial_state)\n",
    "    \n",
    "    # Debug print state\n",
    "    print(\"Final state keys:\", list(final_state.keys()))\n",
    "    \n",
    "    # Check if search_results exists and has content\n",
    "    if \"search_results\" not in final_state or not final_state[\"search_results\"]:\n",
    "        print(\"WARNING: No search results found in final state!\")\n",
    "        # If the search_results got lost, we should check if it's available in our papers_db\n",
    "        if papers_db:\n",
    "            print(f\"Found {len(papers_db)} papers in papers_db, using those instead\")\n",
    "            final_state[\"search_results\"] = list(papers_db.values())\n",
    "    \n",
    "    return final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6581d80",
   "metadata": {},
   "source": [
    "## Display Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c379da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_langgraph_results(results: Dict[str, Any]) -> None:\n",
    "    \"\"\"Display the research results in a formatted way.\n",
    "    \n",
    "    Creates formatted Markdown outputs for the expanded query,\n",
    "    research analysis, and top papers.\n",
    "    \n",
    "    Args:\n",
    "        results: The final state from the research workflow\n",
    "    \"\"\"\n",
    "    from IPython.display import display, Markdown, HTML\n",
    "    \n",
    "    display(Markdown(\"### QUERY EXPANSION\"))\n",
    "    display(Markdown(results[\"expanded_query\"]))\n",
    "    \n",
    "    display(Markdown(\"### RESEARCH ANALYSIS\"))\n",
    "    display(Markdown(results[\"analysis\"]))\n",
    "    \n",
    "    display(Markdown(\"### TOP PAPERS\"))\n",
    "    \n",
    "    # Debug info\n",
    "    display(Markdown(f\"**Debug:** State keys: {list(results.keys())}\"))\n",
    "    \n",
    "    if \"search_results\" not in results or not results[\"search_results\"]:\n",
    "        display(Markdown(\"**No papers found in search results.**\"))\n",
    "    else:\n",
    "        display(Markdown(f\"**Found {len(results['search_results'])} papers.**\"))\n",
    "        for i, paper in enumerate(results[\"search_results\"][:5]):\n",
    "            paper_md = f\"\"\"\n",
    "**{i+1}. {paper['title']}**\n",
    "\n",
    "*Authors:* {paper['authors']}\n",
    "\n",
    "*Published:* {paper['published']}\n",
    "\n",
    "*URL:* {paper['url']}\n",
    "\n",
    "*Abstract:* {paper['abstract'][:300]}...\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "            display(Markdown(paper_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e50e0b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPANSION NODE - Output expanded query: Vision Transformers (ViTs) represent a groundbreaking shift in computer vision, applying the Transfo...\n",
      "SEARCH NODE - Input state keys: ['query', 'expanded_query', 'context', 'messages', 'search_results', 'analysis']\n",
      "Clean search query: \"what are vision transformers?\" OR (\"Vision Transformer\" OR \"ViT\" OR \"self-attention\" OR \"computer vision\")\n",
      "Found 20 papers\n",
      "Processed 20 papers for state\n",
      "SEARCH NODE - Output state keys: ['search_results']\n",
      "Final state keys: ['query', 'expanded_query', 'context', 'messages', 'search_results', 'analysis']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### QUERY EXPANSION"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Vision Transformers (ViTs) represent a groundbreaking shift in computer vision, applying the Transformer architecture, originally designed for natural language processing, to image recognition tasks.  Understanding ViTs involves exploring several key areas:\n",
       "\n",
       "**1. The Core Architecture:**\n",
       "\n",
       "* **Self-Attention Mechanism:**  This is the heart of the Transformer. It allows the model to weigh the importance of different parts of an image (or sequence in NLP) in relation to each other, capturing long-range dependencies.  Key concepts here include query, key, and value embeddings, attention matrices, and multi-head self-attention.\n",
       "* **Transformer Blocks:** These are the building blocks of the ViT. Each block typically consists of a multi-head self-attention layer, followed by a feed-forward network, both wrapped in residual connections and layer normalization.\n",
       "* **Image Patch Embeddings:**  Unlike convolutional networks, ViTs treat images as sequences of patches.  Each image patch is flattened and linearly projected to an embedding vector, analogous to word embeddings in NLP. Positional embeddings are also added to retain spatial information.\n",
       "* **Classification Head:**  After processing through the Transformer blocks, a classification token (CLS token) is used for the final image classification.\n",
       "\n",
       "**2. Comparison to Convolutional Neural Networks (CNNs):**\n",
       "\n",
       "* **Inductive Bias:** CNNs have a built-in inductive bias for locality and translation equivariance. ViTs, however, have a weaker inductive bias, relying more on data to learn these properties. This can be both an advantage and a disadvantage.\n",
       "* **Computational Complexity:** The computational cost of self-attention scales quadratically with the input sequence length. This can be a limitation for high-resolution images.  Various approaches are being developed to mitigate this, such as sparse attention and hierarchical architectures.\n",
       "* **Performance:** ViTs have demonstrated comparable or superior performance to CNNs on various image recognition benchmarks, particularly with large datasets.\n",
       "\n",
       "**3. Variants and Advancements:**\n",
       "\n",
       "* **Data-efficient ViTs:**  Techniques like data augmentation, self-supervised learning, and knowledge distillation are being employed to train ViTs with less labeled data. Examples include DeiT (Data-efficient Image Transformers).\n",
       "* **Hybrid Architectures:** Some models combine the strengths of CNNs and ViTs, using convolutional layers for early feature extraction and transformers for global reasoning.\n",
       "* **Hierarchical ViTs:**  These models process images at multiple scales, offering better computational efficiency and handling larger images effectively.  Swin Transformer and Pyramid Vision Transformer (PVT) are prominent examples.\n",
       "* **Vision Transformer Applications:**  Beyond image classification, ViTs are being applied to various tasks like object detection, image segmentation, video understanding, and image generation.\n",
       "\n",
       "**4. Training and Optimization:**\n",
       "\n",
       "* **Large-scale datasets:** ViTs typically benefit from training on massive datasets.\n",
       "* **Optimizer choices:** AdamW and other optimizers with weight decay are commonly used.\n",
       "* **Learning rate schedules:**  Techniques like linear warm-up and cosine decay are often employed.\n",
       "\n",
       "\n",
       "By understanding these concepts and subtopics, one can gain a comprehensive understanding of Vision Transformers and their implications for the future of computer vision."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESEARCH ANALYSIS"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "These papers explore various aspects of Vision Transformers (ViTs) and address some of their limitations.  \"Vision Transformer: ViT and its Derivatives\" provides a general overview of ViTs and their evolution, setting the stage for subsequent papers. \"A Unified Pruning Framework for Vision Transformers\" tackles the high computational cost and data requirements of ViTs by proposing a pruning method to reduce model size and complexity.  \"Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding\" focuses on improving the robustness of ViTs, specifically addressing architectural differences compared to CNNs and potentially enhancing performance in noisy or adversarial scenarios by modifying the normalization strategy.  \"Vision Transformer with Progressive Sampling\" aims to optimize the computational efficiency of ViTs, likely by strategically sampling input tokens or features, thereby reducing the computational burden of attention mechanisms. Finally, \"Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification\" applies ViTs to the medical imaging domain and enhances robustness through a self-ensembling technique, potentially improving performance and reliability in challenging medical image classification tasks.\n",
       "\n",
       "Key themes emerging from these papers include:\n",
       "\n",
       "* **Computational Efficiency:**  Several papers directly address the high computational cost associated with ViTs, using techniques like pruning and progressive sampling.\n",
       "* **Robustness:**  Improving the robustness of ViTs against noise, adversarial attacks, and data variations is another significant area of focus.\n",
       "* **Application Specificity:**  The application of ViTs to specific domains, such as medical imaging, highlights the adaptability and potential of these architectures.\n",
       "* **Architectural Modifications:** Papers explore modifications to the core ViT architecture, including normalization strategies and embedding methods, to enhance performance and address specific limitations.\n",
       "\n",
       "\n",
       "In summary, these papers demonstrate the ongoing research and development efforts to refine and optimize ViTs, addressing challenges like computational complexity, robustness, and domain-specific application.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### TOP PAPERS"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Debug:** State keys: ['query', 'expanded_query', 'context', 'messages', 'search_results', 'analysis']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Found 20 papers.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**1. Vision Transformer: Vit and its Derivatives**\n",
       "\n",
       "*Authors:* Zujun Fu\n",
       "\n",
       "*Published:* 2022-05-12\n",
       "\n",
       "*URL:* http://arxiv.org/abs/2205.11239v2\n",
       "\n",
       "*Abstract:* Transformer, an attention-based encoder-decoder architecture, has not only\n",
       "revolutionized the field of natural language processing (NLP), but has also\n",
       "done some pioneering work in the field of computer vision (CV). Compared to\n",
       "convolutional neural networks (CNNs), the Vision Transformer (ViT) relies...\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**2. A Unified Pruning Framework for Vision Transformers**\n",
       "\n",
       "*Authors:* Hao Yu, Jianxin Wu\n",
       "\n",
       "*Published:* 2021-11-30\n",
       "\n",
       "*URL:* http://arxiv.org/abs/2111.15127v1\n",
       "\n",
       "*Abstract:* Recently, vision transformer (ViT) and its variants have achieved promising\n",
       "performances in various computer vision tasks. Yet the high computational costs\n",
       "and training data requirements of ViTs limit their application in\n",
       "resource-constrained settings. Model compression is an effective method to\n",
       "spe...\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**3. Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding**\n",
       "\n",
       "*Authors:* Bum Jun Kim, Hyeyeon Choi, Hyeonah Jang, Dong Gu Lee, Wonseok Jeong, Sang Woo Kim\n",
       "\n",
       "*Published:* 2021-11-16\n",
       "\n",
       "*URL:* http://arxiv.org/abs/2111.08413v1\n",
       "\n",
       "*Abstract:* Vision transformers (ViTs) have recently demonstrated state-of-the-art\n",
       "performance in a variety of vision tasks, replacing convolutional neural\n",
       "networks (CNNs). Meanwhile, since ViT has a different architecture than CNN, it\n",
       "may behave differently. To investigate the reliability of ViT, this paper\n",
       "st...\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**4. Vision Transformer with Progressive Sampling**\n",
       "\n",
       "*Authors:* Xiaoyu Yue, Shuyang Sun, Zhanghui Kuang, Meng Wei, Philip Torr, Wayne Zhang, Dahua Lin\n",
       "\n",
       "*Published:* 2021-08-03\n",
       "\n",
       "*URL:* http://arxiv.org/abs/2108.01684v1\n",
       "\n",
       "*Abstract:* Transformers with powerful global relation modeling abilities have been\n",
       "introduced to fundamental computer vision tasks recently. As a typical example,\n",
       "the Vision Transformer (ViT) directly applies a pure transformer architecture\n",
       "on image classification, by simply splitting images into tokens with a...\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**5. Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification**\n",
       "\n",
       "*Authors:* Faris Almalik, Mohammad Yaqub, Karthik Nandakumar\n",
       "\n",
       "*Published:* 2022-08-04\n",
       "\n",
       "*URL:* http://arxiv.org/abs/2208.02851v1\n",
       "\n",
       "*Abstract:* Vision Transformers (ViT) are competing to replace Convolutional Neural\n",
       "Networks (CNN) for various computer vision tasks in medical imaging such as\n",
       "classification and segmentation. While the vulnerability of CNNs to adversarial\n",
       "attacks is a well-known problem, recent works have shown that ViTs are a...\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============= Main Execution =============\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter your research query: \")\n",
    "    results = research_arxiv_langgraph(query)\n",
    "    display_langgraph_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d609d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6122b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "what are vision transformers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6207338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
