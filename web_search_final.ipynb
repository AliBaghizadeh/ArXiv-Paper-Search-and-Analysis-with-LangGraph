{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d45ce147",
   "metadata": {},
   "source": [
    "# Explanation of Key Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee34ba7c",
   "metadata": {},
   "source": [
    "This project is one of my two projects for the **[Google 5-Day GenAI Intensive Course](https://rsvp.withgoogle.com/events/google-generative-ai-intensive_2025q1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262baab2",
   "metadata": {},
   "source": [
    "\n",
    "### Data Collection Pipeline:\n",
    "\n",
    "Uses arXiv API to search for papers based on research queries               \n",
    "Prioritizes HTML versions of papers (available for papers since Dec 2023)               \n",
    "Falls back to abstracts for older papers without HTML versions          \n",
    "Caches paper information to avoid redundant processing            \n",
    "Extracts clean text content using BeautifulSoup for HTML papers     \n",
    "\n",
    "### Query Enhancement:\n",
    "\n",
    "Employs few-shot prompting with domain-specific examples        \n",
    "Expands original queries to include related concepts and terminology         \n",
    "Extracts key terms from expanded queries for more effective searching         \n",
    "Adapts to specific research domains with customized examples          \n",
    "Handles both general research and specialized topics (like Vision Transformers)    \n",
    "\n",
    "### LangGraph RAG Workflow:\n",
    "\n",
    "Implements a structured workflow with defined nodes and transitions       \n",
    "Maintains comprehensive state throughout the research process          \n",
    "Four-stage pipeline: query expansion → paper search → analysis → response generation         \n",
    "Each node updates specific parts of the state without losing information          \n",
    "Handles the full research journey from question to comprehensive analysis      \n",
    "\n",
    "### Paper Analysis Capabilities:\n",
    "\n",
    "Generates in-depth analyses of retrieved papers using few-shot learning       \n",
    "Identifies connections between papers and research question           \n",
    "Extracts key contributions, methodologies, and technical details           \n",
    "Provides research context through carefully selected examples          \n",
    "Synthesizes information across multiple papers for comprehensive understanding          \n",
    "\n",
    "### User Interaction:\n",
    "\n",
    "Provides formatted Markdown output for easy reading in notebooks      \n",
    "Displays expanded query to show understanding of research needs         \n",
    "Presents comprehensive research analysis with insights and connections          \n",
    "Lists top papers with titles, authors, publication dates, and abstracts       \n",
    "Includes direct links to original papers on arXiv          \n",
    "Simple interface for entering research queries and viewing results          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c17ae",
   "metadata": {},
   "source": [
    "## Customizable Parameters in the Pipeline\n",
    "\n",
    "Here are the key parameters to modify to experiment with results:\n",
    "\n",
    "**Model Parameters**     \n",
    "Model Version: Change model = genai.GenerativeModel('models/gemini-1.5-pro-latest') to use a different Gemini model         \n",
    "Temperature: Add temperature parameter when creating the model to control creativity (e.g., model = genai.GenerativeModel('models/gemini-1.5-pro-latest', temperature=0.2))             \n",
    "\n",
    "**Search Parameters**\n",
    "Max Results: Modify max_results=20 in arxiv_api_search() to return more/fewer papers                  \n",
    "Sort Criteria: Change sort_by=arxiv.SortCriterion.Relevance to sort by other criteria like Submitted or LastUpdated             \n",
    "Category Filter: Customize the category filter logic in arxiv_api_search() (currently set to cs.CV for vision-related queries)              \n",
    "\n",
    "**Content Processing**\n",
    "Papers Analyzed: Change the number of papers analyzed in analyze_papers_node() and analyze_papers() (currently uses top 5)             \n",
    "Few-Shot Examples: Modify the examples in expand_research_query() and analyze_papers() to better fit your domain         \n",
    "\n",
    "**Display Settings**\n",
    "Display Limit: Change the slice in results[\"search_results\"][:5] in display_langgraph_results() to show more papers         \n",
    "Abstract Length: Adjust the paper['abstract'][:300] to show more/less text      \n",
    "\n",
    "**Workflow Configuration**\n",
    "Node Ordering: Rearrange the workflow by modifying edges in create_research_workflow()               \n",
    "Initial State: Add additional fields to the initial state in research_arxiv_langgraph()            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2bdbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U google-generativeai langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac61639",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63c0da9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlanggraph  Version: 0.3.34\\ngoogle-generativeai Version: 0.8.5\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============= Imports and Configuration =============\n",
    "\n",
    "import os\n",
    "import arxiv\n",
    "import requests\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import TypedDict, List, Dict, Any, Sequence, Optional, Callable\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# LLM and message handling imports\n",
    "import google.generativeai as genai\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# Vector database imports\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "\"\"\"\n",
    "langgraph  Version: 0.3.34\n",
    "google-generativeai Version: 0.8.5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949084a0",
   "metadata": {},
   "source": [
    "## State and Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b3d0310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized vector database\n"
     ]
    }
   ],
   "source": [
    "class GoogleEmbeddingFunction(chromadb.utils.embedding_functions.EmbeddingFunction):\n",
    "    \"\"\"Custom embedding function that uses Google's Generative AI API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, debug: bool = False):\n",
    "        \"\"\"Initialize with Google API key.\n",
    "        \n",
    "        Args:\n",
    "            api_key: Google API key\n",
    "            debug: Whether to print debug information\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.debug = debug\n",
    "        genai.configure(api_key=api_key)\n",
    "    \n",
    "    def __call__(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for the provided texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts to embed\n",
    "            \n",
    "        Returns:\n",
    "            List of embedding vectors\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for text in texts:\n",
    "            try:\n",
    "                # Truncate text if too long (API has limits)\n",
    "                if len(text) > 8000:\n",
    "                    text = text[:8000]\n",
    "                \n",
    "                if self.debug:\n",
    "                    print(f\"Generating embedding for text of length {len(text)}\")\n",
    "                \n",
    "                # Create embedding request\n",
    "                embedding_response = genai.embed_content(\n",
    "                    model=\"embedding-001\",\n",
    "                    content=text,\n",
    "                    task_type=\"retrieval_document\",\n",
    "                )\n",
    "                \n",
    "                if self.debug:\n",
    "                    print(f\"Embedding response type: {type(embedding_response)}\")\n",
    "                    print(f\"Embedding response attributes: {dir(embedding_response) if hasattr(embedding_response, '__dir__') else 'No attributes'}\")\n",
    "                \n",
    "                # Try to extract embedding based on different possible response structures\n",
    "                if hasattr(embedding_response, \"embedding\"):\n",
    "                    embedding_vector = embedding_response.embedding\n",
    "                    if self.debug:\n",
    "                        print(f\"Found embedding with length: {len(embedding_vector)}\")\n",
    "                elif hasattr(embedding_response, \"embeddings\") and embedding_response.embeddings:\n",
    "                    embedding_vector = embedding_response.embeddings[0]\n",
    "                    if self.debug:\n",
    "                        print(f\"Found embedding in embeddings array with length: {len(embedding_vector)}\")\n",
    "                else:\n",
    "                    # Fallback to dict-style access\n",
    "                    try:\n",
    "                        if isinstance(embedding_response, dict) and \"embedding\" in embedding_response:\n",
    "                            embedding_vector = embedding_response[\"embedding\"]\n",
    "                        elif isinstance(embedding_response, dict) and \"embeddings\" in embedding_response:\n",
    "                            embedding_vector = embedding_response[\"embeddings\"][0]\n",
    "                        else:\n",
    "                            raise ValueError(\"Could not find embedding in response\")\n",
    "                    except Exception as e:\n",
    "                        if self.debug:\n",
    "                            print(f\"Dict access failed: {e}\")\n",
    "                        # Use random fallback\n",
    "                        embedding_vector = np.random.rand(768).tolist()\n",
    "                        print(\"Warning: Using random fallback embedding (dict access failed)\")\n",
    "                \n",
    "                embeddings.append(embedding_vector)\n",
    "                \n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(f\"Error generating embedding (details): {type(e).__name__}: {e}\")\n",
    "                # Fallback: random embeddings\n",
    "                embeddings.append(np.random.rand(768).tolist())\n",
    "                print(\"Warning: Using random fallback embedding due to exception\")\n",
    "                \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# Function to toggle debug output globally\n",
    "DEBUG_MODE = False\n",
    "\n",
    "def set_debug_mode(enabled: bool = True):\n",
    "    \"\"\"Enable or disable debug output globally.\n",
    "    \n",
    "    Args:\n",
    "        enabled: Whether to enable debug output\n",
    "    \"\"\"\n",
    "    global DEBUG_MODE\n",
    "    DEBUG_MODE = enabled\n",
    "    print(f\"Debug mode {'enabled' if enabled else 'disabled'}\")\n",
    "\n",
    "\n",
    "# Create a ChromaDB collection\n",
    "def get_vector_db(debug: bool = False):\n",
    "    \"\"\"Get the vector database client and collection.\n",
    "    \n",
    "    Creates or retrieves a ChromaDB collection for storing paper embeddings\n",
    "    using Google's text embeddings model.\n",
    "    \n",
    "    Args:\n",
    "        debug: Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of (client, collection)\n",
    "    \"\"\"\n",
    "    client = chromadb.Client()\n",
    "    \n",
    "    # Set up our custom embedding function\n",
    "    embedding_function = GoogleEmbeddingFunction(api_key=GOOGLE_API_KEY, debug=debug)\n",
    "    \n",
    "    # Create or get a collection\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=\"arxiv_papers\",\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    \n",
    "    return client, collection\n",
    "\n",
    "# Initialize the vector database\n",
    "try:\n",
    "    client, collection = get_vector_db(debug=DEBUG_MODE)\n",
    "    print(\"Successfully initialized vector database\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing vector database: {e}\")\n",
    "    # Create a fallback in-memory dictionary to store papers\n",
    "    # This will allow the pipeline to run without vector search capability\n",
    "    collection = None\n",
    "    print(\"Using fallback storage without vector search capabilities\")\n",
    "\n",
    "# Papers database cache\n",
    "papers_db = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe80951",
   "metadata": {},
   "source": [
    "## Query Expansion and Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1fcc6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_research_query(query: str) -> str:\n",
    "    \"\"\"Expand a research query using few-shot prompting.\n",
    "    \n",
    "    Uses domain-specific examples to help the model generate\n",
    "    a comprehensive expansion of the original query.\n",
    "    \n",
    "    Args:\n",
    "        query: The original research query\n",
    "        \n",
    "    Returns:\n",
    "        An expanded version of the query with additional concepts and terms\n",
    "    \"\"\"\n",
    "    # Vision transformer specific example if the query is about vision transformers\n",
    "    if \"vision transformer\" in query.lower() or \"vit\" in query.lower():\n",
    "        few_shot_examples = \"\"\"\n",
    "        Example 1:\n",
    "        Query: \"vision transformer architecture\"\n",
    "        Expanded: The query is about Vision Transformer (ViT) architectures for computer vision tasks. Key aspects to explore include: original ViT design and patch-based image tokenization; comparison with CNN architectures; attention mechanisms specialized for vision; hierarchical and pyramid vision transformers; efficiency improvements like token pruning and sparse attention; distillation techniques for vision transformers; adaptations for different vision tasks including detection and segmentation; recent innovations addressing quadratic complexity and attention saturation.\n",
    "        \n",
    "        Example 2: \n",
    "        Query: \"how do vision transformers process images\"\n",
    "        Expanded: The query focuses on the internal mechanisms of how Vision Transformers process visual information. Key areas to investigate include: patch embedding processes; position embeddings for spatial awareness; self-attention mechanisms for global context; the role of MLP blocks in feature transformation; how class tokens aggregate information; patch size impact on performance and efficiency; multi-head attention design in vision applications; information flow through vision transformer layers; differences from convolutional approaches to feature extraction.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        few_shot_examples = \"\"\"\n",
    "        Example 1:\n",
    "        Query: \"transformer models for NLP\"\n",
    "        Expanded: The query is about transformer architecture models used in natural language processing. Key aspects to explore include: BERT, GPT, T5, and other transformer variants; attention mechanisms; self-supervision and pre-training approaches; fine-tuning methods; performance on NLP tasks like translation, summarization, and question answering; efficiency improvements like distillation and pruning; recent innovations in transformer architectures.\n",
    "        \n",
    "        Example 2:\n",
    "        Query: \"reinforcement learning for robotics\"\n",
    "        Expanded: The query concerns applying reinforcement learning methods to robotic systems. Important areas to investigate include: policy gradient methods; Q-learning variants for continuous control; sim-to-real transfer; imitation learning; model-based RL for robotics; sample efficiency techniques; multi-agent RL for coordinated robots; safety constraints in robotic RL; real-world applications and benchmarks; hierarchical RL for complex tasks.\n",
    "        \n",
    "        Example 3:\n",
    "        Query: \"graph neural networks applications\"\n",
    "        Expanded: The query focuses on practical applications of graph neural networks. Key dimensions to explore include: GNN architectures (GCN, GAT, GraphSAGE); applications in chemistry and drug discovery; recommender systems using GNNs; traffic and transportation network modeling; social network analysis; knowledge graph completion; GNNs for computer vision tasks; scalability solutions for large graphs; theoretical foundations of graph representation learning.\n",
    "        \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Based on the examples below, expand my research query to identify key concepts, relevant subtopics, and specific areas to explore:\n",
    "\n",
    "    {few_shot_examples}\n",
    "\n",
    "    Query: \"{query}\"\n",
    "    Expanded:\"\"\"\n",
    "    \n",
    "    generation_config = {\"temperature\": 1.0}\n",
    "    \n",
    "    response = model.generate_content(prompt, generation_config=generation_config)\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "\n",
    "def analyze_papers(query: str, papers: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Analyze papers using few-shot prompting with domain-specific examples.\n",
    "    \n",
    "    Generates a research analysis based on the retrieved papers and\n",
    "    the original query using domain-specific examples.\n",
    "    \n",
    "    Args:\n",
    "        query: The original research query\n",
    "        papers: List of paper dictionaries containing metadata and content\n",
    "        \n",
    "    Returns:\n",
    "        A comprehensive analysis of the papers in relation to the query\n",
    "    \"\"\"\n",
    "    few_shot_examples = \"\"\"\n",
    "    Example 1:\n",
    "    Papers:\n",
    "    1. \"Attention Is All You Need\" - Introduced the transformer architecture relying entirely on attention mechanisms without recurrence or convolutions.\n",
    "    2. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" - Proposed bidirectional training for transformers using masked language modeling.\n",
    "    \n",
    "    Analysis:\n",
    "    These papers represent seminal work in transformer architectures for NLP. \"Attention Is All You Need\" established the foundation with the original transformer design using multi-head self-attention. BERT built upon this by introducing bidirectional context modeling and masked language modeling for pre-training, significantly advancing performance on downstream tasks. Key themes include attention mechanisms, pre-training objectives, and the importance of training methodology.\n",
    "    \n",
    "    Example 2:\n",
    "    Query: \"How does the Less-Attention Vision Transformer architecture address the computational inefficiencies and saturation problems of traditional Vision Transformers?\"\n",
    "    Papers: \n",
    "    1. \"You Only Need Less Attention at Each Stage in Vision Transformers\" - Proposed reusing early-layer attention scores through linear transformations to reduce computational costs.\n",
    "    \n",
    "    Analysis:\n",
    "    Less-Attention Vision Transformer reduces ViT's quadratic attention cost by reusing early-layer attention scores through linear transformations. It also mitigates attention saturation using residual downsampling and a custom loss to preserve attention structure. This approach addresses two key limitations of traditional Vision Transformers: computational inefficiency due to quadratic complexity of self-attention, and the saturation problem where attention maps become increasingly similar in deeper layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format paper information\n",
    "    paper_info = \"\\n\".join([\n",
    "        f\"{i+1}. \\\"{p['title']}\\\" - {p['abstract'][:200]}...\" \n",
    "        for i, p in enumerate(papers[:5])\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"Based on the examples below, analyze the following research papers related to \"{query}\" to identify key technical contributions, methodologies, and how they address specific challenges:\n",
    "\n",
    "    {few_shot_examples}\n",
    "    \n",
    "    Papers:\n",
    "    {paper_info}\n",
    "    \n",
    "    Analysis:\"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt, generation_config={\"temperature\": 1.0})\n",
    "    \n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812cc982",
   "metadata": {},
   "source": [
    "## arXiv Data Collection Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64cf7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_api_search(query: str, max_results: int = 20) -> List[Any]:\n",
    "    \"\"\"Search arXiv for papers matching the query.\n",
    "    \n",
    "    Uses the arXiv API to find relevant papers based on the query,\n",
    "    with optional filtering for specific categories.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        max_results: Maximum number of results to return (default: 20)\n",
    "        \n",
    "    Returns:\n",
    "        List of arxiv.Result objects representing papers\n",
    "    \"\"\"\n",
    "    # Use category filter for more relevant results\n",
    "    category_filter = \"cat:cs.CV\" if \"vision\" in query.lower() or \"image\" in query.lower() else \"\"\n",
    "    search_query = f\"{query} {category_filter}\".strip()\n",
    "    \n",
    "    # Create a Client instance and use it for search\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query=search_query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    return list(client.results(search))\n",
    "\n",
    "\n",
    "def check_html_available(paper_id: str) -> bool:\n",
    "    \"\"\"Check if HTML version is available for a paper.\n",
    "    \n",
    "    Tests if the paper has an HTML version on arXiv by\n",
    "    sending a HEAD request to the HTML URL.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: The arXiv paper ID\n",
    "        \n",
    "    Returns:\n",
    "        True if HTML version is available, False otherwise\n",
    "    \"\"\"\n",
    "    html_url = f\"https://arxiv.org/html/{paper_id}\"\n",
    "    response = requests.head(html_url)\n",
    "    return response.status_code == 200\n",
    "\n",
    "\n",
    "def get_html_content(paper_id: str) -> Optional[str]:\n",
    "    \"\"\"Get HTML content of a paper if available.\n",
    "    \n",
    "    Fetches and parses the HTML version of a paper, removing\n",
    "    irrelevant elements and extracting the main content.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: The arXiv paper ID\n",
    "        \n",
    "    Returns:\n",
    "        Extracted text content from the HTML version or None if unavailable\n",
    "    \"\"\"\n",
    "    html_url = f\"https://arxiv.org/html/{paper_id}\"\n",
    "    response = requests.get(html_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Remove scripts, styles, and navigation elements\n",
    "        for tag in soup(['script', 'style', 'nav', 'header', 'footer']):\n",
    "            tag.decompose()\n",
    "        # Get main content\n",
    "        main_content = soup.find('main') or soup.find('body')\n",
    "        if main_content:\n",
    "            return main_content.get_text(separator='\\n', strip=True)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576e276",
   "metadata": {},
   "source": [
    "## Vector Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe4eb71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_paper_to_vector_db(paper_info: Dict[str, Any]) -> None:\n",
    "    \"\"\"Add a paper to the vector database.\n",
    "    \n",
    "    Stores a paper's metadata and content in the vector database\n",
    "    for semantic search.\n",
    "    \n",
    "    Args:\n",
    "        paper_info: Dictionary containing paper metadata and content\n",
    "    \"\"\"\n",
    "    # Skip if collection is not available\n",
    "    if collection is None:\n",
    "        print(f\"Skipping vector DB storage for: {paper_info['title']} (collection not available)\")\n",
    "        return\n",
    "        \n",
    "    # Use a combination of title and content for embedding\n",
    "    text_to_embed = f\"{paper_info['title']} {paper_info['content'][:5000]}\"\n",
    "    \n",
    "    # Add to the collection\n",
    "    try:\n",
    "        collection.add(\n",
    "            ids=[paper_info['paper_id']],\n",
    "            documents=[text_to_embed],\n",
    "            metadatas=[{\n",
    "                'title': paper_info['title'],\n",
    "                'authors': paper_info['authors'],\n",
    "                'published': paper_info['published'],\n",
    "                'url': paper_info['url'],\n",
    "                'abstract': paper_info['abstract']\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        print(f\"Added paper to vector DB: {paper_info['title']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding paper to vector DB: {e}\")\n",
    "\n",
    "\n",
    "# Alternative embedding function that doesn't use ChromaDB's interface\n",
    "def get_text_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get embedding for a single text using Google's API.\n",
    "    \n",
    "    This function can be used outside of ChromaDB for testing.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to embed\n",
    "        \n",
    "    Returns:\n",
    "        Embedding vector or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(text) > 8000:\n",
    "            text = text[:8000]\n",
    "            \n",
    "        result = genai.embed_content(\n",
    "            model=\"embedding-001\",\n",
    "            content=text,\n",
    "            task_type=\"retrieval_document\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Direct embedding result: {result}\")\n",
    "        print(f\"Result type: {type(result)}\")\n",
    "        print(f\"Result has embedding attr: {hasattr(result, 'embedding')}\")\n",
    "        \n",
    "        if hasattr(result, 'embedding'):\n",
    "            return result.embedding\n",
    "        else:\n",
    "            # Try dict-style access\n",
    "            try:\n",
    "                if isinstance(result, dict) and \"embedding\" in result:\n",
    "                    return result[\"embedding\"]\n",
    "                else:\n",
    "                    return None\n",
    "            except:\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"Direct embedding error: {type(e).__name__}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def query_vector_db(query: str, n_results: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Query the vector database for papers semantically similar to the query.\n",
    "    \n",
    "    Performs a semantic search in the vector database to find\n",
    "    papers most relevant to the query.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        n_results: Maximum number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of paper dictionaries containing metadata\n",
    "    \"\"\"\n",
    "    # Skip if collection is not available\n",
    "    if collection is None:\n",
    "        print(\"Vector DB search not available, using keyword search only\")\n",
    "        return []\n",
    "        \n",
    "    try:\n",
    "        # Query the collection\n",
    "        results = collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        papers = []\n",
    "        \n",
    "        if results and 'ids' in results and len(results['ids'][0]) > 0:\n",
    "            for i in range(len(results['ids'][0])):\n",
    "                paper_id = results['ids'][0][i]\n",
    "                \n",
    "                # Skip if no metadata (shouldn't happen, but just in case)\n",
    "                if 'metadatas' not in results or i >= len(results['metadatas'][0]):\n",
    "                    continue\n",
    "                    \n",
    "                metadata = results['metadatas'][0][i]\n",
    "                \n",
    "                # Get the full paper info from our database\n",
    "                if paper_id in papers_db:\n",
    "                    papers.append(papers_db[paper_id])\n",
    "                else:\n",
    "                    # Reconstruct from metadata if not in cache\n",
    "                    papers.append({\n",
    "                        'paper_id': paper_id,\n",
    "                        'title': metadata.get('title', 'Unknown Title'),\n",
    "                        'authors': metadata.get('authors', 'Unknown Authors'),\n",
    "                        'published': metadata.get('published', 'Unknown Date'),\n",
    "                        'url': metadata.get('url', ''),\n",
    "                        'abstract': metadata.get('abstract', ''),\n",
    "                        'content': metadata.get('abstract', '')  # Fall back to abstract\n",
    "                    })\n",
    "        \n",
    "        return papers\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying vector DB: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84063ed7",
   "metadata": {},
   "source": [
    "## LangGraph Workflow Nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e2b6cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_expansion_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"LangGraph node that expands the original query.\n",
    "    \n",
    "    Takes the original query and generates an expanded version\n",
    "    with additional concepts and search terms.\n",
    "    \n",
    "    Args:\n",
    "        state: The current LangGraph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with expanded query\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    # Use a very explicit prompt to avoid the model repeating our instructions\n",
    "    prompt = f\"\"\"\n",
    "    Please expand the following research query:\n",
    "    \n",
    "    Query: \"{query}\"\n",
    "    \n",
    "    Provide a detailed expansion that identifies key concepts, \n",
    "    terminology, and relevant subtopics. Do not include phrases like\n",
    "    \"Query:\" or \"Expanded:\" in your response. Just provide the expanded content.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    expanded_query = response.text.strip()\n",
    "    \n",
    "    print(\"EXPANSION NODE - Output expanded query:\", expanded_query[:100] + \"...\")\n",
    "    \n",
    "    return {\"expanded_query\": expanded_query}\n",
    "\n",
    "\n",
    "def search_papers_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"LangGraph node that searches for papers based on the query.\n",
    "    \n",
    "    Uses the original and expanded query to search arXiv for relevant\n",
    "    papers, processes them, and stores the results in both the regular\n",
    "    database and the vector database.\n",
    "    \n",
    "    Args:\n",
    "        state: The current LangGraph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with search results\n",
    "    \"\"\"\n",
    "    print(\"SEARCH NODE - Input state keys:\", list(state.keys()))\n",
    "    \n",
    "    query = state[\"query\"]\n",
    "    expanded_query = state[\"expanded_query\"]\n",
    "    \n",
    "    # Extract actual search terms from expanded query\n",
    "    # First, check if expanded query starts with \"Query:\" (which would indicate our formatting issue)\n",
    "    if \"Query:\" in expanded_query and \"Expanded:\" in expanded_query:\n",
    "        # Extract just the expansion part\n",
    "        expanded_query = expanded_query.split(\"Expanded:\")[1].strip()\n",
    "    \n",
    "    # Extract key terms based on domain\n",
    "    domain_specific_terms = []\n",
    "    \n",
    "    # Domain detection logic\n",
    "    if \"vision transformer\" in query.lower() or \"vit\" in query.lower():\n",
    "        domain_specific_terms = [\"Vision Transformer\", \"ViT\", \"image patches\", \n",
    "                          \"self-attention\", \"transformer encoder\", \n",
    "                          \"multi-head attention\", \"computer vision\"]\n",
    "    elif \"graph\" in query.lower() and \"neural\" in query.lower():\n",
    "        domain_specific_terms = [\"Graph Neural Network\", \"GNN\", \"node embedding\",\n",
    "                          \"message passing\", \"graph attention\", \"GraphSAGE\"]\n",
    "    # Add more domain detection as needed\n",
    "    elif \"reinforcement learning\" in query.lower() or \" rl \" in f\" {query.lower()} \":\n",
    "        domain_specific_terms = [\"Reinforcement Learning\", \"RL\", \"policy gradient\", \n",
    "                          \"Q-learning\", \"reward function\", \"MDP\", \"Markov Decision Process\", \n",
    "                          \"DDPG\", \"PPO\", \"TD learning\", \"actor-critic\"]\n",
    "    elif \"large language model\" in query.lower() or \"llm\" in query.lower():\n",
    "        domain_specific_terms = [\"Large Language Model\", \"LLM\", \"transformer\", \n",
    "                          \"attention mechanism\", \"GPT\", \"BERT\", \"prompt engineering\", \n",
    "                          \"fine-tuning\", \"few-shot learning\", \"instruction tuning\"]\n",
    "    elif \"diffusion\" in query.lower() and (\"model\" in query.lower() or \"image\" in query.lower()):\n",
    "        domain_specific_terms = [\"Diffusion Model\", \"DDPM\", \"latent diffusion\", \n",
    "                          \"score-based generative model\", \"noise prediction\", \n",
    "                          \"reverse diffusion\", \"U-Net\", \"text-to-image\"]\n",
    "    elif \"robotics\" in query.lower() or \"robot\" in query.lower():\n",
    "        domain_specific_terms = [\"Robotics\", \"robot learning\", \"manipulation\", \n",
    "                          \"grasping\", \"trajectory optimization\", \"inverse kinematics\", \n",
    "                          \"motion planning\", \"control policy\", \"sim2real\"]\n",
    "    elif \"recommendation\" in query.lower() or \"recommender\" in query.lower():\n",
    "        domain_specific_terms = [\"Recommender System\", \"collaborative filtering\", \n",
    "                          \"content-based filtering\", \"matrix factorization\", \n",
    "                          \"user embedding\", \"item embedding\", \"CTR prediction\"]\n",
    "    elif \"computer vision\" in query.lower() or \"image\" in query.lower():\n",
    "        domain_specific_terms = [\"Computer Vision\", \"CNN\", \"object detection\", \n",
    "                          \"segmentation\", \"image recognition\", \"feature extraction\", \n",
    "                          \"SIFT\", \"ResNet\", \"Faster R-CNN\", \"YOLO\"]\n",
    "    elif (\"natural language\" in query.lower() or \"nlp\" in query.lower()) and \"transformer\" not in query.lower():\n",
    "        domain_specific_terms = [\"Natural Language Processing\", \"NLP\", \"named entity recognition\", \n",
    "                          \"sentiment analysis\", \"text classification\", \"word embedding\", \n",
    "                          \"language model\", \"sequence-to-sequence\", \"LSTM\", \"RNN\"]\n",
    "    elif \"generative\" in query.lower() or \"gan\" in query.lower():\n",
    "        domain_specific_terms = [\"Generative Adversarial Network\", \"GAN\", \"StyleGAN\", \n",
    "                          \"generator\", \"discriminator\", \"adversarial training\", \n",
    "                          \"latent space\", \"mode collapse\", \"image synthesis\"]\n",
    "    elif \"attention\" in query.lower() or \"transformer\" in query.lower():\n",
    "        domain_specific_terms = [\"Transformer\", \"attention mechanism\", \"self-attention\", \n",
    "                          \"multi-head attention\", \"encoder-decoder\", \"positional encoding\", \n",
    "                          \"cross-attention\", \"attention weights\"]\n",
    "    elif \"quantum\" in query.lower() and (\"computing\" in query.lower() or \"machine learning\" in query.lower()):\n",
    "        domain_specific_terms = [\"Quantum Computing\", \"quantum machine learning\", \n",
    "                          \"quantum circuit\", \"qubit\", \"quantum gate\", \"variational quantum circuit\", \n",
    "                          \"QAOA\", \"quantum advantage\", \"quantum supremacy\"]\n",
    "    \n",
    "    # Generic ML terms for any ML-related query\n",
    "    if any(term in query.lower() for term in [\"machine learning\", \"neural network\", \"deep learning\", \"ai\"]):\n",
    "        generic_ml_terms = [\"neural network\", \"deep learning\", \"backpropagation\", \n",
    "                     \"gradient descent\", \"loss function\", \"activation function\", \n",
    "                     \"hyperparameter tuning\", \"regularization\", \"overfitting\"]\n",
    "        domain_specific_terms.extend(generic_ml_terms)\n",
    "    \n",
    "    # If no specific domain is detected, extract key terms from the expanded query\n",
    "    if not domain_specific_terms and expanded_query:\n",
    "        # Extract potential terms from expanded query\n",
    "        expanded_lines = expanded_query.split('. ')\n",
    "        for line in expanded_lines:\n",
    "            # Find capitalized terms or terms in quotes that might be important concepts\n",
    "            import re\n",
    "            potential_terms = re.findall(r'([A-Z][a-zA-Z0-9]+([ \\-][A-Z][a-zA-Z0-9]+)*)', line)\n",
    "            quoted_terms = re.findall(r'\"([^\"]+)\"', line)\n",
    "            \n",
    "            # Add these as domain terms\n",
    "            for term in potential_terms:\n",
    "                if isinstance(term, tuple):\n",
    "                    term = term[0]  # Extract the actual term from regex match tuple\n",
    "                if len(term) > 3 and term not in domain_specific_terms:  # Only terms longer than 3 chars\n",
    "                    domain_specific_terms.append(term)\n",
    "                    \n",
    "            domain_specific_terms.extend(quoted_terms)\n",
    "    \n",
    "    # Log the detected terms\n",
    "    if domain_specific_terms:\n",
    "        print(f\"Detected domain terms: {domain_specific_terms}\")\n",
    "    else:\n",
    "        print(\"No specific domain terms detected, using original query only\")\n",
    "    \n",
    "    # Create a clean search query by combining original and expanded\n",
    "    search_query = query\n",
    "    if domain_specific_terms:\n",
    "        expanded_terms = \" OR \".join(f'\"{term}\"' for term in domain_specific_terms)\n",
    "        search_query = f'\"{query}\" OR ({expanded_terms})'\n",
    "    \n",
    "    print(f\"Clean search query: {search_query}\")\n",
    "    \n",
    "    # Regular search via arXiv API\n",
    "    papers = arxiv_api_search(search_query)\n",
    "    print(f\"Found {len(papers)} papers via API search\")\n",
    "    \n",
    "    # Process and store papers\n",
    "    results = []\n",
    "    for paper in papers:\n",
    "        paper_id = paper.entry_id.split('/')[-1]\n",
    "        \n",
    "        # Skip if already in database\n",
    "        if paper_id in papers_db:\n",
    "            results.append(papers_db[paper_id])\n",
    "            continue\n",
    "            \n",
    "        paper_info = {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"title\": paper.title,\n",
    "            \"authors\": \", \".join(author.name for author in paper.authors),\n",
    "            \"published\": paper.published.strftime(\"%Y-%m-%d\"),\n",
    "            \"url\": paper.entry_id,\n",
    "            \"abstract\": paper.summary,\n",
    "            \"has_html\": check_html_available(paper_id)\n",
    "        }\n",
    "        \n",
    "        # Get HTML content if available\n",
    "        if paper_info[\"has_html\"]:\n",
    "            paper_info[\"content\"] = get_html_content(paper_id)\n",
    "        else:\n",
    "            paper_info[\"content\"] = paper_info[\"abstract\"]\n",
    "            \n",
    "        # Store in our database\n",
    "        papers_db[paper_id] = paper_info\n",
    "        \n",
    "        # Add to vector database\n",
    "        add_paper_to_vector_db(paper_info)\n",
    "        \n",
    "        results.append(paper_info)\n",
    "    \n",
    "    print(f\"Processed {len(results)} papers for state\")\n",
    "    \n",
    "    # Now perform semantic search\n",
    "    semantic_results = query_vector_db(query)\n",
    "    print(f\"Found {len(semantic_results)} papers via semantic search\")\n",
    "    \n",
    "    # Combine results but prioritize semantic search results\n",
    "    combined_results = []\n",
    "    \n",
    "    # First add semantic results\n",
    "    paper_ids_added = set()\n",
    "    for paper in semantic_results:\n",
    "        combined_results.append(paper)\n",
    "        paper_ids_added.add(paper[\"paper_id\"])\n",
    "    \n",
    "    # Then add any API results not already included\n",
    "    for paper in results:\n",
    "        if paper[\"paper_id\"] not in paper_ids_added:\n",
    "            combined_results.append(paper)\n",
    "            paper_ids_added.add(paper[\"paper_id\"])\n",
    "    \n",
    "    # Return updated state\n",
    "    updated_state = {\n",
    "        \"search_results\": combined_results[:10],  # Limit to top 10 papers\n",
    "        \"embedding_results\": semantic_results\n",
    "    }\n",
    "    print(\"SEARCH NODE - Output state keys:\", list(updated_state.keys()))\n",
    "    return updated_state\n",
    "\n",
    "\n",
    "def analyze_papers_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"LangGraph node that analyzes papers.\n",
    "    \n",
    "    Takes the search results and generates an analysis\n",
    "    of the papers in relation to the original query.\n",
    "    \n",
    "    Args:\n",
    "        state: The current LangGraph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with paper analysis and context\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    search_results = state[\"search_results\"]\n",
    "    \n",
    "    # Get the vector search results if available\n",
    "    embedding_results = state.get(\"embedding_results\", [])\n",
    "    \n",
    "    # Prioritize embedding results if available\n",
    "    papers_to_analyze = embedding_results if embedding_results else search_results\n",
    "    \n",
    "    analysis = analyze_papers(query, papers_to_analyze)\n",
    "    \n",
    "    # Extract relevant content for context\n",
    "    context = []\n",
    "    for paper in papers_to_analyze[:5]:\n",
    "        context.append(f\"Title: {paper['title']}\\nAuthors: {paper['authors']}\\nAbstract: {paper['abstract']}\")\n",
    "    \n",
    "    return {\n",
    "        \"analysis\": analysis,\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_response_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"LangGraph node that generates the final response.\n",
    "    \n",
    "    Formats the analysis and adds it to the message history\n",
    "    in the state.\n",
    "    \n",
    "    Args:\n",
    "        state: The current LangGraph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with messages\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    context = state[\"context\"]\n",
    "    analysis = state[\"analysis\"]\n",
    "    \n",
    "    # Format the message as an AI response\n",
    "    message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": analysis\n",
    "    }\n",
    "    \n",
    "    # Add the message to the state\n",
    "    if \"messages\" not in state:\n",
    "        state[\"messages\"] = []\n",
    "    \n",
    "    state[\"messages\"].append(message)\n",
    "    \n",
    "    return {\"messages\": state[\"messages\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ffc61f",
   "metadata": {},
   "source": [
    "## LangGraph Workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7bf47f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_research_workflow():\n",
    "    \"\"\"Create a LangGraph workflow for research.\n",
    "    \n",
    "    Defines the workflow graph with nodes for query expansion,\n",
    "    paper search, analysis, and response generation.\n",
    "    \n",
    "    Returns:\n",
    "        A compiled LangGraph workflow\n",
    "    \"\"\"\n",
    "    # Initialize the workflow with the RAGState\n",
    "    workflow = StateGraph(RAGState)\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    workflow.add_node(\"query_expansion\", query_expansion_node)\n",
    "    workflow.add_node(\"search_papers\", search_papers_node)\n",
    "    workflow.add_node(\"analyze_papers\", analyze_papers_node)\n",
    "    workflow.add_node(\"generate_response\", generate_response_node)\n",
    "    \n",
    "    # Add edges to connect the nodes\n",
    "    workflow.add_edge(\"query_expansion\", \"search_papers\")\n",
    "    workflow.add_edge(\"search_papers\", \"analyze_papers\")\n",
    "    workflow.add_edge(\"analyze_papers\", \"generate_response\")\n",
    "    \n",
    "    # Set the entry point\n",
    "    workflow.set_entry_point(\"query_expansion\")\n",
    "    \n",
    "    # Compile the workflow\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c51291",
   "metadata": {},
   "source": [
    "## Interface Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "63a5ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_arxiv_langgraph(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Research arXiv papers using the LangGraph workflow with embeddings.\n",
    "    \n",
    "    Main function that executes the full research pipeline\n",
    "    on a given query, using both keyword and semantic search.\n",
    "    \n",
    "    Args:\n",
    "        query: The research query\n",
    "        \n",
    "    Returns:\n",
    "        The final state with all results\n",
    "    \"\"\"\n",
    "    # Create the workflow\n",
    "    workflow = create_research_workflow()\n",
    "    \n",
    "    # Initialize the state\n",
    "    initial_state = {\n",
    "        \"query\": query,\n",
    "        \"expanded_query\": \"\",\n",
    "        \"context\": [],\n",
    "        \"messages\": [],\n",
    "        \"search_results\": [],\n",
    "        \"embedding_results\": [],\n",
    "        \"analysis\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Execute the workflow\n",
    "    final_state = workflow.invoke(initial_state)\n",
    "    \n",
    "    # Debug print state\n",
    "    print(\"Final state keys:\", list(final_state.keys()))\n",
    "    \n",
    "    # Check if search_results exists and has content\n",
    "    if \"search_results\" not in final_state or not final_state[\"search_results\"]:\n",
    "        print(\"WARNING: No search results found in final state!\")\n",
    "        # If the search_results got lost, we should check if it's available in our papers_db\n",
    "        if papers_db:\n",
    "            print(f\"Found {len(papers_db)} papers in papers_db, using those instead\")\n",
    "            final_state[\"search_results\"] = list(papers_db.values())\n",
    "    \n",
    "    return final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6581d80",
   "metadata": {},
   "source": [
    "## Display Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c379da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_langgraph_results(results: Dict[str, Any]) -> None:\n",
    "    \"\"\"Display the research results in a formatted way.\n",
    "    \n",
    "    Creates formatted Markdown outputs for the expanded query,\n",
    "    research analysis, and top papers, with separate sections\n",
    "    for semantic search results.\n",
    "    \n",
    "    Args:\n",
    "        results: The final state from the research workflow\n",
    "    \"\"\"\n",
    "    from IPython.display import display, Markdown, HTML\n",
    "    \n",
    "    display(Markdown(\"### QUERY EXPANSION\"))\n",
    "    display(Markdown(results[\"expanded_query\"]))\n",
    "    \n",
    "    display(Markdown(\"### RESEARCH ANALYSIS\"))\n",
    "    display(Markdown(results[\"analysis\"]))\n",
    "    \n",
    "    # Show vector-based results first\n",
    "    if \"embedding_results\" in results and results[\"embedding_results\"]:\n",
    "        display(Markdown(\"### TOP PAPERS (SEMANTIC SEARCH)\"))\n",
    "        display(Markdown(f\"**Found {len(results['embedding_results'])} papers via semantic search.**\"))\n",
    "        \n",
    "        for i, paper in enumerate(results[\"embedding_results\"][:3]):\n",
    "            paper_md = f\"\"\"\n",
    "                        **{i+1}. {paper['title']}**\n",
    "\n",
    "                        *Authors:* {paper['authors']}\n",
    "\n",
    "                        *Published:* {paper['published']}\n",
    "\n",
    "                        *URL:* {paper['url']}\n",
    "\n",
    "                        *Abstract:* {paper['abstract'][:300]}...\n",
    "\n",
    "                        ---\n",
    "                        \"\"\"\n",
    "            display(Markdown(paper_md))\n",
    "    \n",
    "    # Show all results\n",
    "    display(Markdown(\"### ALL TOP PAPERS\"))\n",
    "    \n",
    "    if \"search_results\" not in results or not results[\"search_results\"]:\n",
    "        display(Markdown(\"**No papers found in search results.**\"))\n",
    "    else:\n",
    "        display(Markdown(f\"**Found {len(results['search_results'])} papers total.**\"))\n",
    "        for i, paper in enumerate(results[\"search_results\"][:5]):\n",
    "            paper_md = f\"\"\"\n",
    "            **{i+1}. {paper['title']}**\n",
    "\n",
    "            *Authors:* {paper['authors']}\n",
    "\n",
    "            *Published:* {paper['published']}\n",
    "\n",
    "            *URL:* {paper['url']}\n",
    "\n",
    "            *Abstract:* {paper['abstract'][:300]}...\n",
    "\n",
    "            ---\n",
    "            \"\"\"\n",
    "            display(Markdown(paper_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b1b72e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct embedding result: {'embedding': [0.015156699, -0.038795594, -0.07823806, 0.0079901805, 0.053636488, 0.040569972, 0.043250248, 0.003482838, -0.029988255, 0.05139732, 0.011054967, 0.013684313, -0.05106629, 0.009752501, 0.009917302, -0.0255604, 0.0010003495, 0.06536013, 0.025839183, -0.013252059, 0.056396417, -0.030313445, 0.027129037, -0.02792571, -0.0068604816, 0.0022004254, -0.0014990492, -0.02060917, -0.039787635, 0.034023985, -0.041221187, 0.007934667, -0.0812762, 0.017318036, 0.041002095, -0.026629968, -0.017167687, -0.015562067, 0.019599516, 0.027103651, 0.0049131066, -0.0083201025, -0.010509021, 0.008437044, -0.0008671561, -0.014161377, -0.012568938, 0.04924568, 0.011494052, -0.032690555, 0.06834012, -0.007718818, 0.06611068, -0.014475824, 0.009712358, -0.044311065, 0.01497575, -0.029068926, -0.011033472, -0.019836018, 0.004582517, 0.008940155, -0.013714338, 0.005951202, -0.04516122, -0.025896149, -0.06906359, 0.04318617, 0.058472615, -0.010477542, -0.019604584, -0.044862155, 0.09124893, -0.01695808, 0.031912513, -0.096633725, 0.002825661, 0.025082825, -0.010886248, 0.018066794, -0.0051442184, -0.060470626, -0.061283123, 0.012716527, -0.06606356, 0.00595917, -0.03811573, -0.023840955, -0.017112806, 0.033100765, -0.027840484, -0.038629536, 0.07807018, -0.042217713, 0.0076017026, 0.039159853, 0.01127786, -0.07232046, 0.0043689366, -0.03619515, -0.009231418, -0.011254392, -0.063123085, 0.01109695, 0.052458417, -0.008441944, 0.0017655231, 0.042919148, -0.013808509, 0.014345085, -0.037959617, 0.0035338306, 0.020581827, 0.029154887, 0.042707507, -0.01376876, -0.006000263, 0.055762835, 0.01933205, 0.04610423, 0.0067088995, -0.032753665, 0.06940212, -0.03295919, -0.02016329, 0.00981437, -0.024178898, 0.017473953, 0.029576814, 0.03439019, -0.02350436, -0.033662904, -0.07171854, 0.021751339, 0.03971409, 0.106315345, 0.0196577, 0.0053580166, 0.057111915, 0.021131456, -0.0042745313, -0.034907907, 0.04629446, 0.05670063, -0.03344843, 0.035617165, -0.047905482, 0.0155763365, 0.018044537, -0.037146274, -0.029706737, -0.021678992, -0.04777059, 0.0068975333, 0.06540459, -0.01374578, -0.0440661, 0.03317416, 0.027022086, 0.04278749, 0.013476272, -0.020475898, 0.007353112, 0.036035284, 0.019704124, -0.06227768, -0.014643673, -0.015637904, -0.0045980685, 0.013221855, -0.0019869583, 0.040024474, -0.0276503, -0.037062615, -0.016419997, -0.048263527, 0.022850037, -0.016800553, -0.03699919, 0.018077374, 0.004233977, -0.024995485, 0.011050568, 0.026601497, -0.028972775, -0.06495962, 0.05589148, -0.0013931483, -0.020978514, 0.021137428, 0.0017546036, -0.020022092, -0.018323138, -0.03066777, -0.016564684, 0.019861946, 0.03457801, -0.02523083, 0.013285879, -0.03869665, -0.02309759, 0.11553963, -0.0029200513, 0.0115278745, 0.008386648, 0.004310962, 0.025865544, 0.0007629668, -0.057534166, 0.03330374, -0.054469496, 0.027079163, 0.0038426057, 0.01937009, 0.033571955, -0.023785302, 0.0308423, 0.037833635, -0.0093797855, -0.034944136, -0.015548564, -0.030139225, -0.021474125, 0.019615885, 0.03264264, 0.07254454, -0.0062413258, 0.021823106, 0.020211507, -0.035297096, -0.011206868, 0.09379216, 0.028958581, -0.02288346, 0.101032175, 0.011248231, 0.025226371, 0.0028467793, 0.0049643847, 0.017242825, -0.038456447, 0.010492987, 0.049671337, 0.026475841, -0.056369714, -0.03753316, -0.043608658, 0.050615698, 0.0042035272, 0.032887615, -0.004737505, -0.0723954, -0.023228113, 0.0029849755, -0.064621426, 0.025570244, -0.045750096, 0.0058030686, -0.06321238, -0.023727413, 0.016036864, 0.011005261, 0.004747733, -0.01527355, -0.0064519565, -0.0023484174, -0.024320863, -0.059615824, 0.023547467, 0.021049483, 0.011077592, -0.06073201, 0.013456159, -0.009120112, 0.012919442, -0.021975372, 0.030880503, 0.046205185, 0.005421081, -0.03432693, 0.013596708, 0.0059381123, 0.04701224, -0.022184871, -0.004550845, 0.012465571, -0.033366352, -0.015281311, 0.02314424, -0.08290825, -0.04110343, -0.020961119, 0.018717865, -0.056048293, -0.042850513, -0.012099361, -0.01569918, 0.03132141, 0.011344985, -0.043443672, -0.018643206, -0.060861334, -0.029810555, -0.107232034, 0.010112424, 0.029569397, -0.029365996, -0.05775566, 0.041805968, 0.042150002, 0.0031580238, 0.0025236828, -0.041747335, -0.036187563, 0.054032996, 0.027614566, -0.024450472, -0.0016246794, 0.018750982, 0.056436535, -0.025124187, 0.021653255, -0.0058327327, -0.009463595, -0.012837591, 0.014922748, 0.0017659006, -0.009675472, -0.02131497, 0.0197263, -0.037711874, 0.029088156, -0.030430835, 0.032508116, 0.01501144, -0.00990805, -0.08044693, 0.006454076, -0.021214388, -0.027224662, 0.018795839, 0.023691375, -0.003470425, -0.035837, 0.009892062, -0.03683084, -0.04401152, 0.020333141, 0.081466995, 0.011859391, 0.006942859, 0.07601574, -0.03169311, 0.03822953, 0.022257634, -0.06380637, 0.05552043, -0.06300049, 0.001739556, -0.051235836, 0.0098346025, 0.053650293, 0.010018494, -0.005214284, 0.0022689542, -0.008838757, -0.021150092, -0.017165815, -0.013876628, 0.05880334, 0.05858479, -0.020071274, 0.018948503, -0.041899804, 0.05077343, -0.0124381, -0.060696468, -0.035360627, 0.02406904, -0.0024614404, -0.01436891, 0.029795378, 0.03722881, 0.017305287, -0.00011404193, -0.048214402, 0.03331687, 0.0152463475, -0.025290204, 0.043114033, -0.049195036, 0.011110632, 0.089537926, 0.01296448, -0.0040312153, -0.027682386, 0.029903838, -0.052256662, 0.010657128, 0.04761012, -0.015427952, -0.065411314, -0.02042505, -0.010830978, -0.016226599, 0.015473546, 0.0071847243, -0.026361223, -0.016644944, -0.0060097366, -0.0072236084, -0.0075087054, 0.017246312, -0.04600548, -0.062959254, 0.007645167, -0.0031691494, -0.058224, -0.0026310403, -0.0070014875, -0.027740564, -0.02032914, -0.0026909825, 0.0052012824, -0.05467533, -0.033450197, 0.021651912, 0.023835683, 0.0064662416, 0.05705133, 0.05049085, -0.025223674, -0.017455582, -0.0048095025, 0.052684825, -0.046804693, -0.009827528, 0.02353006, -0.03364964, 0.007769615, 0.021231215, -0.01516081, -0.012834654, -0.021129308, -0.052948296, 0.0024781208, 0.023188911, -0.017518828, 0.018939326, -0.11542992, 0.0190015, -0.037164927, -0.021867037, -0.07723852, -0.028531084, -0.04343345, 0.00822584, 0.06932625, -0.018090114, 0.0014480122, 0.058282677, -0.040231697, -0.022967165, -0.06628885, 0.06609821, -0.015706198, -0.010442019, -0.016611991, 0.02250532, 0.03909902, -0.0018877648, -0.024728926, 0.0077053625, -0.023137089, -0.07254061, -0.016819688, -0.093547486, 0.06098441, -0.05121517, 0.04224323, 0.020944318, 0.0010093166, -0.031191453, 0.016247889, -0.04314467, -0.003257693, 0.040478047, 0.009495271, 0.0072524766, 0.017887302, 0.025250979, 0.0029289967, -0.0017241718, -0.04128612, 0.0034006191, 0.020641504, -0.0064948965, 0.04132294, 0.06164657, 0.012532561, -0.020527048, -0.044556044, 0.014659348, 0.025740065, 0.045433894, -0.06307572, 0.012277742, 0.016502865, -0.016978797, -0.018142406, 0.049810488, 0.04305863, 0.029964617, 0.01059709, 0.07467182, -0.051589422, 0.017989224, 0.048900165, -0.033333443, -0.00624805, -0.021770844, 0.0021585436, -0.09089598, -0.04385239, 0.01752859, 0.011144889, -0.009082015, 0.0008567197, -0.08053636, 0.012173342, -0.06593926, 0.07512886, -0.013882978, -0.0024906052, 0.045303173, -0.00689459, 0.03752067, 0.014046237, -0.013521108, -0.045325186, 0.018116435, -0.017386558, 0.022474976, -0.00038251, -0.027713154, 0.010959619, 0.006996204, -0.07103443, 0.0011661206, -0.0039530452, -0.022130867, 0.0013288711, 0.0030409668, -0.025089635, 0.041970827, -0.019847283, 0.00567136, 0.03525879, -0.015229827, -0.042157385, 0.023599168, 0.03377473, 0.020976795, 0.033105716, 0.04497263, 0.028193489, -0.05960191, -0.033821344, 0.061254185, 0.004730802, 0.006468502, 0.0015477049, 0.042115975, 0.027178772, 0.03611235, -0.006681017, -0.049390446, -0.007319648, -0.0058123176, 0.014440024, 0.043073773, -0.020448677, -0.008572103, 0.057462066, 0.055512298, -0.015462115, 0.05697754, 0.009948855, 0.02692933, -0.00056062755, -0.046507597, 0.03160195, -0.024254045, -0.044209175, 0.033563513, 0.0049974336, -0.0040207775, -0.03334963, -0.04710939, -0.02163468, 0.011877519, -0.023571774, 0.03241881, -0.024207711, 0.026173832, 0.02202517, -0.0013135034, 0.004633739, 0.020838886, 0.049941756, -0.026334573, -0.034456212, 0.0014667945, -0.00047202827, -0.035106104, -0.039148524, 0.07876512, 0.0036983844, -0.005273443, -0.058472823, -0.04077034, 0.035882056, 0.022678066, -0.011380184, 0.007276278, 0.02002348, 0.0001595158, -0.036849137, 0.06370398, 0.027223771, 0.02775395, 0.06505992, 0.0060418835, 0.012497684, -0.016762877, -0.014157963, -0.024171116, 0.06625865, 0.0032358496, -0.01654237, -0.06390183, -0.030027179, -0.004596081, -0.020787098, 0.005411184, 0.084265836, 0.0067341486, -0.06396499, -0.06827411, 0.027052525, -0.026978528, 0.027080517, 0.0041424166, -0.012189589, 0.032098163, 0.06229147, -0.016142078, -0.045525085, 0.034049846, -0.026760273, -0.025098424, -9.834732e-05, 0.002545567, 0.009811493, -0.014947365, -0.03759023, -0.035322957, -0.09721527, -0.022362402, 0.010229208, -0.069664925, -0.01017999, 0.050714243, -0.022851767, 0.025251161, 0.018789854, -0.038057916, 0.040701866, 0.041326687, 0.016720539, 0.021310166, 0.016746147, -0.048290294, -0.009614875, -0.0059745465, 0.011003905, 0.061491698, -0.011169375, -0.037404206, -0.062100526, 0.013100681, -0.025678093, -0.012799074, 0.026476137, 0.019043736, 0.0037720099, -0.022653112, 0.0058849775, 0.04358355, 0.014870479, 0.0055559855, -0.05263765, -0.00018715889, 0.035252385, 0.011339496, -0.022559976, 0.030144896, 0.009564477, 0.033474043, 0.06835912, 0.07853615, -0.055056192, -0.016375408, 0.03898871, 0.03559178, -0.023746409, 0.010586271, 0.042257607, -0.04673576, 0.06272743, 0.02122653, 0.026382834, 0.05076419, -0.036523815, -0.09702631, 0.06168914, -0.031403594, -0.041067876, -0.0096492795, 0.008094815, 0.053051025, -0.011651954, -0.015921257, 0.065740466, -0.009981512, 0.11230766, 0.043344542, 0.024146002, 0.015245304, -0.09256264, 0.00462504, 0.014903751, 0.01986301, 0.06983805, 0.030031756, -0.03392042, -0.02545792, -0.054865003, 0.0030101195, -0.082415335, -0.036046874, -0.01714654, -0.07526183, 0.04346906, 0.05799309, 0.013465779, 0.010174017, -0.049572192, -0.02162935, -0.006703368, -0.0074557606, 0.007370397, 0.017164048, -0.019087235, -0.0009127948, 0.017053492, -0.0006852061, 0.02095627]}\n",
      "Result type: <class 'dict'>\n",
      "Result has embedding attr: False\n",
      "Successfully generated embedding with length 768\n",
      "First few values: [0.015156699, -0.038795594, -0.07823806, 0.0079901805, 0.053636488]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Vision Transformers (ViT) are a type of neural network architecture\"\n",
    "embedding = get_text_embedding(test_text)\n",
    "if embedding:\n",
    "    print(f\"Successfully generated embedding with length {len(embedding)}\")\n",
    "    print(f\"First few values: {embedding[:5]}\")\n",
    "else:\n",
    "    print(\"Failed to generate embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e50e0b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug mode disabled\n",
      "==================================================\n",
      "arXiv Research Pipeline with Embedding-Based Search\n",
      "==================================================\n",
      "\n",
      "Searching arXiv and analyzing papers...\n",
      "EXPANSION NODE - Output expanded query: Vision Transformers (ViTs) apply the Transformer architecture, originally designed for Natural Langu...\n",
      "SEARCH NODE - Input state keys: ['query', 'expanded_query', 'context', 'messages', 'search_results', 'analysis', 'embedding_results']\n",
      "Detected domain terms: ['Vision Transformer', 'ViT', 'image patches', 'self-attention', 'transformer encoder', 'multi-head attention', 'computer vision']\n",
      "Clean search query: \"what are vision transformers?\" OR (\"Vision Transformer\" OR \"ViT\" OR \"image patches\" OR \"self-attention\" OR \"transformer encoder\" OR \"multi-head attention\" OR \"computer vision\")\n",
      "Found 20 papers via API search\n",
      "Processed 20 papers for state\n",
      "Found 5 papers via semantic search\n",
      "SEARCH NODE - Output state keys: ['search_results', 'embedding_results']\n",
      "Final state keys: ['query', 'expanded_query', 'context', 'messages', 'search_results', 'analysis', 'embedding_results']\n",
      "\n",
      "Displaying research results:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### QUERY EXPANSION"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Vision Transformers (ViTs) apply the Transformer architecture, originally designed for Natural Language Processing (NLP), to computer vision tasks.  This involves treating images as sequences of patches, similar to words in a sentence.  Understanding ViTs requires exploring several key concepts:\n",
       "\n",
       "* **Transformer Architecture:** This architecture relies heavily on the **self-attention mechanism**, which allows the model to weigh the importance of different parts of the input sequence (image patches in the case of ViTs) when processing it.  Key components include:\n",
       "    * **Self-Attention:** Calculates relationships between all patches in an image to capture global context.\n",
       "    * **Multi-Head Self-Attention:** Employs multiple self-attention mechanisms operating in parallel, allowing the model to capture different relationships between patches.\n",
       "    * **Positional Encoding:**  Since Transformers don't inherently understand the spatial arrangement of patches, positional encodings are added to provide information about the location of each patch within the image.\n",
       "    * **Encoder-Decoder Structure:** While the original Transformer uses both an encoder and a decoder, ViTs for image classification typically use only the encoder.\n",
       "    * **Feedforward Networks:**  MLP layers within each encoder block process the output of the self-attention mechanism.\n",
       "    * **Layer Normalization:** Normalizes the activations within each layer to improve training stability.\n",
       "\n",
       "* **Image Patchification:**  The process of dividing an image into smaller, non-overlapping patches which are then linearly projected into a lower-dimensional embedding space.  The size of these patches is a hyperparameter.\n",
       "\n",
       "* **Comparison with Convolutional Neural Networks (CNNs):**  ViTs offer an alternative to CNNs, which have been the dominant architecture in computer vision for many years. Exploring the differences in inductive biases, computational complexity, and performance characteristics between ViTs and CNNs is crucial.\n",
       "\n",
       "* **Hybrid Architectures:**  Some models combine the strengths of both CNNs and Transformers.  These hybrid architectures might use CNNs for initial feature extraction and then employ a Transformer for global context aggregation.\n",
       "\n",
       "* **Applications of Vision Transformers:**  ViTs are being used for a variety of computer vision tasks beyond image classification, including:\n",
       "    * **Object Detection:** Identifying and localizing objects within an image.\n",
       "    * **Image Segmentation:**  Partitioning an image into meaningful regions.\n",
       "    * **Image Generation:**  Creating new images.\n",
       "    * **Video Processing:** Analyzing and understanding video content.\n",
       "\n",
       "* **Advantages and Disadvantages of ViTs:**  Understanding the trade-offs associated with using ViTs is important.  Advantages can include better performance on large datasets and the ability to capture long-range dependencies.  Disadvantages can include the need for large datasets for training and higher computational cost compared to some CNN architectures.\n",
       "\n",
       "* **Training and Optimization:**  Training ViTs effectively requires careful consideration of optimization strategies, data augmentation techniques, and regularization methods.\n",
       "\n",
       "* **Variants of Vision Transformers:** Several modifications and improvements have been proposed to the original ViT architecture.  Researching these variants, such as DeiT, Swin Transformer, and Pyramid Vision Transformer, is essential for a comprehensive understanding.\n",
       "\n",
       "\n",
       "By exploring these key concepts and subtopics, one can gain a thorough understanding of Vision Transformers and their role in the evolving landscape of computer vision."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESEARCH ANALYSIS"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "These papers explore different facets of applying and improving Vision Transformers (ViTs).  They address challenges related to data efficiency, computational cost, and specific applications.\n",
       "\n",
       "* **\"Where are my Neighbors?...\":** This paper tackles the **data efficiency** problem of ViTs on smaller datasets.  It leverages the relationships between image patches, likely through some form of local attention or patch comparison, to improve performance when sufficient training data isn't available.  This addresses the common issue of ViTs requiring large datasets for effective training.\n",
       "\n",
       "* **\"Content-aware Token Sharing...\":** This work focuses on **computational efficiency** for semantic segmentation with ViTs.  The proposed Content-aware Token Sharing (CTS) method reduces the number of tokens processed, likely by merging similar tokens based on their content.  This directly addresses the computational bottleneck of processing a large number of tokens, especially in dense prediction tasks like segmentation.\n",
       "\n",
       "* **\"EDTER: Edge Detection with Transformer\":** This paper explores a specific application of ViTs to **edge detection**.  It likely leverages the attention mechanism to capture long-range dependencies and contextual information crucial for accurate edge identification, potentially addressing limitations of CNNs in preserving local details while capturing global context.\n",
       "\n",
       "* **\"Vision Transformer for Classification of Breast Ultrasound Images\":**  This research applies ViTs to a specific medical imaging task: **breast ultrasound image classification**. It explores the potential of ViTs in this domain, possibly comparing performance with traditional CNN-based methods. This contributes to the growing body of work evaluating ViT's effectiveness in various medical image analysis applications.\n",
       "\n",
       "* **\"ViT-1.58b: Mobile Vision Transformers in the 1-bit Era\":** This paper addresses the **computational cost** of ViTs, specifically targeting mobile deployments.  The focus on \"1-bit\" suggests the use of quantization techniques to reduce memory footprint and computational complexity, enabling efficient deployment of large ViT models on resource-constrained devices.  This tackles the challenge of deploying powerful ViT models in real-world applications with limited resources.\n",
       "\n",
       "\n",
       "In summary, these papers contribute to the advancement of ViTs by addressing key challenges such as data efficiency, computational cost, and exploring novel applications.  They employ methodologies like exploiting patch relationships, token reduction, 1-bit quantization, and applying transformers to specific tasks like edge detection and medical image analysis.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### TOP PAPERS (SEMANTIC SEARCH)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Found 5 papers via semantic search.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "                        **1. Where are my Neighbors? Exploiting Patches Relations in Self-Supervised Vision Transformer**\n",
       "\n",
       "                        *Authors:* Guglielmo Camporese, Elena Izzo, Lamberto Ballan\n",
       "\n",
       "                        *Published:* 2022-06-01\n",
       "\n",
       "                        *URL:* http://arxiv.org/abs/2206.00481v2\n",
       "\n",
       "                        *Abstract:* Vision Transformers (ViTs) enabled the use of the transformer architecture on\n",
       "vision tasks showing impressive performances when trained on big datasets.\n",
       "However, on relatively small datasets, ViTs are less accurate given their lack\n",
       "of inductive bias. To this end, we propose a simple but still effect...\n",
       "\n",
       "                        ---\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "                        **2. Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers**\n",
       "\n",
       "                        *Authors:* Chenyang Lu, Daan de Geus, Gijs Dubbelman\n",
       "\n",
       "                        *Published:* 2023-06-03\n",
       "\n",
       "                        *URL:* http://arxiv.org/abs/2306.02095v1\n",
       "\n",
       "                        *Abstract:* This paper introduces Content-aware Token Sharing (CTS), a token reduction\n",
       "approach that improves the computational efficiency of semantic segmentation\n",
       "networks that use Vision Transformers (ViTs). Existing works have proposed\n",
       "token reduction approaches to improve the efficiency of ViT-based image\n",
       "c...\n",
       "\n",
       "                        ---\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "                        **3. EDTER: Edge Detection with Transformer**\n",
       "\n",
       "                        *Authors:* Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan, Haibin Ling\n",
       "\n",
       "                        *Published:* 2022-03-16\n",
       "\n",
       "                        *URL:* http://arxiv.org/abs/2203.08566v1\n",
       "\n",
       "                        *Abstract:* Convolutional neural networks have made significant progresses in edge\n",
       "detection by progressively exploring the context and semantic features.\n",
       "However, local details are gradually suppressed with the enlarging of receptive\n",
       "fields. Recently, vision transformer has shown excellent capability in\n",
       "captur...\n",
       "\n",
       "                        ---\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ALL TOP PAPERS"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Found 10 papers total.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "            **1. Where are my Neighbors? Exploiting Patches Relations in Self-Supervised Vision Transformer**\n",
       "\n",
       "            *Authors:* Guglielmo Camporese, Elena Izzo, Lamberto Ballan\n",
       "\n",
       "            *Published:* 2022-06-01\n",
       "\n",
       "            *URL:* http://arxiv.org/abs/2206.00481v2\n",
       "\n",
       "            *Abstract:* Vision Transformers (ViTs) enabled the use of the transformer architecture on\n",
       "vision tasks showing impressive performances when trained on big datasets.\n",
       "However, on relatively small datasets, ViTs are less accurate given their lack\n",
       "of inductive bias. To this end, we propose a simple but still effect...\n",
       "\n",
       "            ---\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "            **2. Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers**\n",
       "\n",
       "            *Authors:* Chenyang Lu, Daan de Geus, Gijs Dubbelman\n",
       "\n",
       "            *Published:* 2023-06-03\n",
       "\n",
       "            *URL:* http://arxiv.org/abs/2306.02095v1\n",
       "\n",
       "            *Abstract:* This paper introduces Content-aware Token Sharing (CTS), a token reduction\n",
       "approach that improves the computational efficiency of semantic segmentation\n",
       "networks that use Vision Transformers (ViTs). Existing works have proposed\n",
       "token reduction approaches to improve the efficiency of ViT-based image\n",
       "c...\n",
       "\n",
       "            ---\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "            **3. EDTER: Edge Detection with Transformer**\n",
       "\n",
       "            *Authors:* Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan, Haibin Ling\n",
       "\n",
       "            *Published:* 2022-03-16\n",
       "\n",
       "            *URL:* http://arxiv.org/abs/2203.08566v1\n",
       "\n",
       "            *Abstract:* Convolutional neural networks have made significant progresses in edge\n",
       "detection by progressively exploring the context and semantic features.\n",
       "However, local details are gradually suppressed with the enlarging of receptive\n",
       "fields. Recently, vision transformer has shown excellent capability in\n",
       "captur...\n",
       "\n",
       "            ---\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "            **4. Vision Transformer for Classification of Breast Ultrasound Images**\n",
       "\n",
       "            *Authors:* Behnaz Gheflati, Hassan Rivaz\n",
       "\n",
       "            *Published:* 2021-10-27\n",
       "\n",
       "            *URL:* http://arxiv.org/abs/2110.14731v3\n",
       "\n",
       "            *Abstract:* Medical ultrasound (US) imaging has become a prominent modality for breast\n",
       "cancer imaging due to its ease-of-use, low-cost and safety. In the past decade,\n",
       "convolutional neural networks (CNNs) have emerged as the method of choice in\n",
       "vision applications and have shown excellent potential in automatic\n",
       "...\n",
       "\n",
       "            ---\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "            **5. ViT-1.58b: Mobile Vision Transformers in the 1-bit Era**\n",
       "\n",
       "            *Authors:* Zhengqing Yuan, Rong Zhou, Hongyi Wang, Lifang He, Yanfang Ye, Lichao Sun\n",
       "\n",
       "            *Published:* 2024-06-26\n",
       "\n",
       "            *URL:* http://arxiv.org/abs/2406.18051v1\n",
       "\n",
       "            *Abstract:* Vision Transformers (ViTs) have achieved remarkable performance in various\n",
       "image classification tasks by leveraging the attention mechanism to process\n",
       "image patches as tokens. However, the high computational and memory demands of\n",
       "ViTs pose significant challenges for deployment in resource-constraine...\n",
       "\n",
       "            ---\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============= Main Execution =============\n",
    "if __name__ == \"__main__\":\n",
    "    # Control verbosity - set to True if you want debug output\n",
    "    set_debug_mode(False)\n",
    "    \n",
    "    # Clear intro\n",
    "    print(\"=\" * 50)\n",
    "    print(\"arXiv Research Pipeline with Embedding-Based Search\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Ask for research query\n",
    "    query = input(\"Enter your research query: \")\n",
    "    \n",
    "    # Run the research pipeline\n",
    "    print(\"\\nSearching arXiv and analyzing papers...\")\n",
    "    results = research_arxiv_langgraph(query)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nDisplaying research results:\\n\")\n",
    "    display_langgraph_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d609d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6122b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "what are vision transformers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6207338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
