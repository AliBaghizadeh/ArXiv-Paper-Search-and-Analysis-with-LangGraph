{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d45ce147",
   "metadata": {},
   "source": [
    "# Explanation of Key Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee34ba7c",
   "metadata": {},
   "source": [
    "**This version does not use Embedings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262baab2",
   "metadata": {},
   "source": [
    "\n",
    "### Data Collection Pipeline:\n",
    "\n",
    "Uses arXiv API to search for papers based on research queries               \n",
    "Prioritizes HTML versions of papers (available for papers since Dec 2023)               \n",
    "Falls back to abstracts for older papers without HTML versions          \n",
    "Caches paper information to avoid redundant processing            \n",
    "Extracts clean text content using BeautifulSoup for HTML papers     \n",
    "\n",
    "### Query Enhancement:\n",
    "\n",
    "Employs few-shot prompting with domain-specific examples        \n",
    "Expands original queries to include related concepts and terminology         \n",
    "Extracts key terms from expanded queries for more effective searching         \n",
    "Adapts to specific research domains with customized examples          \n",
    "Handles both general research and specialized topics (like Vision Transformers)    \n",
    "\n",
    "### LangGraph RAG Workflow:\n",
    "\n",
    "Implements a structured workflow with defined nodes and transitions       \n",
    "Maintains comprehensive state throughout the research process          \n",
    "Four-stage pipeline: query expansion → paper search → analysis → response generation         \n",
    "Each node updates specific parts of the state without losing information          \n",
    "Handles the full research journey from question to comprehensive analysis      \n",
    "\n",
    "### Paper Analysis Capabilities:\n",
    "\n",
    "Generates in-depth analyses of retrieved papers using few-shot learning       \n",
    "Identifies connections between papers and research question           \n",
    "Extracts key contributions, methodologies, and technical details           \n",
    "Provides research context through carefully selected examples          \n",
    "Synthesizes information across multiple papers for comprehensive understanding          \n",
    "\n",
    "### User Interaction:\n",
    "\n",
    "Provides formatted Markdown output for easy reading in notebooks      \n",
    "Displays expanded query to show understanding of research needs         \n",
    "Presents comprehensive research analysis with insights and connections          \n",
    "Lists top papers with titles, authors, publication dates, and abstracts       \n",
    "Includes direct links to original papers on arXiv          \n",
    "Simple interface for entering research queries and viewing results          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c17ae",
   "metadata": {},
   "source": [
    "## Customizable Parameters in the Pipeline\n",
    "\n",
    "Here are the key parameters to modify to experiment with results:\n",
    "\n",
    "**Model Parameters**     \n",
    "Model Version: Change model = genai.GenerativeModel('models/gemini-1.5-pro-latest') to use a different Gemini model         \n",
    "Temperature: Add temperature parameter when creating the model to control creativity (e.g., model = genai.GenerativeModel('models/gemini-1.5-pro-latest', temperature=0.2))             \n",
    "\n",
    "**Search Parameters**\n",
    "Max Results: Modify max_results=20 in arxiv_api_search() to return more/fewer papers                  \n",
    "Sort Criteria: Change sort_by=arxiv.SortCriterion.Relevance to sort by other criteria like Submitted or LastUpdated             \n",
    "Category Filter: Customize the category filter logic in arxiv_api_search() (currently set to cs.CV for vision-related queries)              \n",
    "\n",
    "**Content Processing**\n",
    "Papers Analyzed: Change the number of papers analyzed in analyze_papers_node() and analyze_papers() (currently uses top 5)             \n",
    "Few-Shot Examples: Modify the examples in expand_research_query() and analyze_papers() to better fit your domain         \n",
    "\n",
    "**Display Settings**\n",
    "Display Limit: Change the slice in results[\"search_results\"][:5] in display_langgraph_results() to show more papers         \n",
    "Abstract Length: Adjust the paper['abstract'][:300] to show more/less text      \n",
    "\n",
    "**Workflow Configuration**\n",
    "Node Ordering: Rearrange the workflow by modifying edges in create_research_workflow()               \n",
    "Initial State: Add additional fields to the initial state in research_arxiv_langgraph()            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2bdbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U google-generativeai langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac61639",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63c0da9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlanggraph  Version: 0.3.34\\ngoogle-generativeai Version: 0.8.5\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============= Imports and Configuration =============\n",
    "\n",
    "import os\n",
    "import arxiv\n",
    "import requests\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import TypedDict, List, Dict, Any, Sequence, Optional, Callable\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# LLM and message handling imports\n",
    "import google.generativeai as genai\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# Vector database imports\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "\"\"\"\n",
    "langgraph  Version: 0.3.34\n",
    "google-generativeai Version: 0.8.5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949084a0",
   "metadata": {},
   "source": [
    "## State and Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b3d0310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized vector database\n"
     ]
    }
   ],
   "source": [
    "class RAGState(TypedDict):\n",
    "    \"\"\"State for the RAG workflow.\n",
    "    \n",
    "    Attributes:\n",
    "        query: The original user query\n",
    "        expanded_query: Query after expansion with the LLM\n",
    "        context: List of contextualized information from papers\n",
    "        messages: List of chat messages in the conversation\n",
    "        search_results: List of papers retrieved from arXiv\n",
    "        analysis: Generated analysis of the papers\n",
    "        embedding_results: Papers retrieved via vector search\n",
    "    \"\"\"\n",
    "    query: str\n",
    "    expanded_query: str\n",
    "    context: List[str]\n",
    "    messages: List[Dict[str, Any]]\n",
    "    search_results: List[Dict[str, Any]]\n",
    "    analysis: str\n",
    "    embedding_results: List[Dict[str, Any]]\n",
    "\n",
    "\n",
    "# API Configuration\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\", \"AIzaSyBuYf9Sdm8M8tIMvfArkcS_YUjhEZfZqes\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Create a model instance\n",
    "model = genai.GenerativeModel('models/gemini-1.5-pro-latest')\n",
    "\n",
    "# Create a custom embedding function for ChromaDB\n",
    "class GoogleEmbeddingFunction(chromadb.utils.embedding_functions.EmbeddingFunction):\n",
    "    \"\"\"Custom embedding function that uses Google's Generative AI API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize with Google API key.\n",
    "        \n",
    "        Args:\n",
    "            api_key: Google API key\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "    \n",
    "    def __call__(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for the provided texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts to embed\n",
    "            \n",
    "        Returns:\n",
    "            List of embedding vectors\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for text in texts:\n",
    "            try:\n",
    "                # Truncate text if too long (API has limits)\n",
    "                if len(text) > 8000:\n",
    "                    text = text[:8000]\n",
    "                \n",
    "                # Use Google's embedding model via global function\n",
    "                embedding_result = genai.embed_content(\n",
    "                    model=\"embedding-001\",\n",
    "                    content=text,\n",
    "                    task_type=\"retrieval_document\",\n",
    "                )\n",
    "                \n",
    "                # Get the embedding values\n",
    "                if embedding_result and hasattr(embedding_result, \"embedding\"):\n",
    "                    embeddings.append(embedding_result.embedding)\n",
    "                else:\n",
    "                    # Fallback: random embeddings if API fails\n",
    "                    print(f\"Warning: Failed to get embedding, using random fallback\")\n",
    "                    embeddings.append(np.random.rand(768).tolist())\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating embedding: {e}\")\n",
    "                # Fallback: random embeddings\n",
    "                embeddings.append(np.random.rand(768).tolist())\n",
    "                \n",
    "        return embeddings\n",
    "\n",
    "# Create a ChromaDB collection\n",
    "def get_vector_db():\n",
    "    \"\"\"Get the vector database client and collection.\n",
    "    \n",
    "    Creates or retrieves a ChromaDB collection for storing paper embeddings\n",
    "    using Google's text embeddings model.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple of (client, collection)\n",
    "    \"\"\"\n",
    "    client = chromadb.Client()\n",
    "    \n",
    "    # Set up our custom embedding function\n",
    "    embedding_function = GoogleEmbeddingFunction(api_key=GOOGLE_API_KEY)\n",
    "    \n",
    "    # Create or get a collection\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=\"arxiv_papers\",\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    \n",
    "    return client, collection\n",
    "\n",
    "# Initialize the vector database\n",
    "try:\n",
    "    client, collection = get_vector_db()\n",
    "    print(\"Successfully initialized vector database\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing vector database: {e}\")\n",
    "    # Create a fallback in-memory dictionary to store papers\n",
    "    # This will allow the pipeline to run without vector search capability\n",
    "    collection = None\n",
    "    print(\"Using fallback storage without vector search capabilities\")\n",
    "\n",
    "# Papers database cache\n",
    "papers_db = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe80951",
   "metadata": {},
   "source": [
    "## Query Expansion and Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fcc6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_research_query(query: str) -> str:\n",
    "    \"\"\"Expand a research query using few-shot prompting.\n",
    "    \n",
    "    Uses domain-specific examples to help the model generate\n",
    "    a comprehensive expansion of the original query.\n",
    "    \n",
    "    Args:\n",
    "        query: The original research query\n",
    "        \n",
    "    Returns:\n",
    "        An expanded version of the query with additional concepts and terms\n",
    "    \"\"\"\n",
    "    # Vision transformer specific example if the query is about vision transformers\n",
    "    if \"vision transformer\" in query.lower() or \"vit\" in query.lower():\n",
    "        few_shot_examples = \"\"\"\n",
    "        Example 1:\n",
    "        Query: \"vision transformer architecture\"\n",
    "        Expanded: The query is about Vision Transformer (ViT) architectures for computer vision tasks. Key aspects to explore include: original ViT design and patch-based image tokenization; comparison with CNN architectures; attention mechanisms specialized for vision; hierarchical and pyramid vision transformers; efficiency improvements like token pruning and sparse attention; distillation techniques for vision transformers; adaptations for different vision tasks including detection and segmentation; recent innovations addressing quadratic complexity and attention saturation.\n",
    "        \n",
    "        Example 2: \n",
    "        Query: \"how do vision transformers process images\"\n",
    "        Expanded: The query focuses on the internal mechanisms of how Vision Transformers process visual information. Key areas to investigate include: patch embedding processes; position embeddings for spatial awareness; self-attention mechanisms for global context; the role of MLP blocks in feature transformation; how class tokens aggregate information; patch size impact on performance and efficiency; multi-head attention design in vision applications; information flow through vision transformer layers; differences from convolutional approaches to feature extraction.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        few_shot_examples = \"\"\"\n",
    "        Example 1:\n",
    "        Query: \"transformer models for NLP\"\n",
    "        Expanded: The query is about transformer architecture models used in natural language processing. Key aspects to explore include: BERT, GPT, T5, and other transformer variants; attention mechanisms; self-supervision and pre-training approaches; fine-tuning methods; performance on NLP tasks like translation, summarization, and question answering; efficiency improvements like distillation and pruning; recent innovations in transformer architectures.\n",
    "        \n",
    "        Example 2:\n",
    "        Query: \"reinforcement learning for robotics\"\n",
    "        Expanded: The query concerns applying reinforcement learning methods to robotic systems. Important areas to investigate include: policy gradient methods; Q-learning variants for continuous control; sim-to-real transfer; imitation learning; model-based RL for robotics; sample efficiency techniques; multi-agent RL for coordinated robots; safety constraints in robotic RL; real-world applications and benchmarks; hierarchical RL for complex tasks.\n",
    "        \n",
    "        Example 3:\n",
    "        Query: \"graph neural networks applications\"\n",
    "        Expanded: The query focuses on practical applications of graph neural networks. Key dimensions to explore include: GNN architectures (GCN, GAT, GraphSAGE); applications in chemistry and drug discovery; recommender systems using GNNs; traffic and transportation network modeling; social network analysis; knowledge graph completion; GNNs for computer vision tasks; scalability solutions for large graphs; theoretical foundations of graph representation learning.\n",
    "        \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Based on the examples below, expand my research query to identify key concepts, relevant subtopics, and specific areas to explore:\n",
    "\n",
    "    {few_shot_examples}\n",
    "\n",
    "    Query: \"{query}\"\n",
    "    Expanded:\"\"\"\n",
    "    \n",
    "    generation_config = {\"temperature\": 1.0}\n",
    "    \n",
    "    response = model.generate_content(prompt, generation_config=generation_config)\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "\n",
    "def analyze_papers(query: str, papers: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Analyze papers using few-shot prompting with domain-specific examples.\n",
    "    \n",
    "    Generates a research analysis based on the retrieved papers and\n",
    "    the original query using domain-specific examples.\n",
    "    \n",
    "    Args:\n",
    "        query: The original research query\n",
    "        papers: List of paper dictionaries containing metadata and content\n",
    "        \n",
    "    Returns:\n",
    "        A comprehensive analysis of the papers in relation to the query\n",
    "    \"\"\"\n",
    "    few_shot_examples = \"\"\"\n",
    "    Example 1:\n",
    "    Papers:\n",
    "    1. \"Attention Is All You Need\" - Introduced the transformer architecture relying entirely on attention mechanisms without recurrence or convolutions.\n",
    "    2. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" - Proposed bidirectional training for transformers using masked language modeling.\n",
    "    \n",
    "    Analysis:\n",
    "    These papers represent seminal work in transformer architectures for NLP. \"Attention Is All You Need\" established the foundation with the original transformer design using multi-head self-attention. BERT built upon this by introducing bidirectional context modeling and masked language modeling for pre-training, significantly advancing performance on downstream tasks. Key themes include attention mechanisms, pre-training objectives, and the importance of training methodology.\n",
    "    \n",
    "    Example 2:\n",
    "    Query: \"How does the Less-Attention Vision Transformer architecture address the computational inefficiencies and saturation problems of traditional Vision Transformers?\"\n",
    "    Papers: \n",
    "    1. \"You Only Need Less Attention at Each Stage in Vision Transformers\" - Proposed reusing early-layer attention scores through linear transformations to reduce computational costs.\n",
    "    \n",
    "    Analysis:\n",
    "    Less-Attention Vision Transformer reduces ViT's quadratic attention cost by reusing early-layer attention scores through linear transformations. It also mitigates attention saturation using residual downsampling and a custom loss to preserve attention structure. This approach addresses two key limitations of traditional Vision Transformers: computational inefficiency due to quadratic complexity of self-attention, and the saturation problem where attention maps become increasingly similar in deeper layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format paper information\n",
    "    paper_info = \"\\n\".join([\n",
    "        f\"{i+1}. \\\"{p['title']}\\\" - {p['abstract'][:200]}...\" \n",
    "        for i, p in enumerate(papers[:5])\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"Based on the examples below, analyze the following research papers related to \"{query}\" to identify key technical contributions, methodologies, and how they address specific challenges:\n",
    "\n",
    "    {few_shot_examples}\n",
    "    \n",
    "    Papers:\n",
    "    {paper_info}\n",
    "    \n",
    "    Analysis:\"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt, generation_config={\"temperature\": 1.0})\n",
    "    \n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812cc982",
   "metadata": {},
   "source": [
    "## arXiv Data Collection Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64cf7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_api_search(query: str, max_results: int = 20) -> List[Any]:\n",
    "    \"\"\"Search arXiv for papers matching the query.\n",
    "    \n",
    "    Uses the arXiv API to find relevant papers based on the query,\n",
    "    with optional filtering for specific categories.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        max_results: Maximum number of results to return (default: 20)\n",
    "        \n",
    "    Returns:\n",
    "        List of arxiv.Result objects representing papers\n",
    "    \"\"\"\n",
    "    # Use category filter for more relevant results\n",
    "    category_filter = \"cat:cs.CV\" if \"vision\" in query.lower() or \"image\" in query.lower() else \"\"\n",
    "    search_query = f\"{query} {category_filter}\".strip()\n",
    "    \n",
    "    # Create a Client instance and use it for search\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query=search_query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    return list(client.results(search))\n",
    "\n",
    "\n",
    "def check_html_available(paper_id: str) -> bool:\n",
    "    \"\"\"Check if HTML version is available for a paper.\n",
    "    \n",
    "    Tests if the paper has an HTML version on arXiv by\n",
    "    sending a HEAD request to the HTML URL.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: The arXiv paper ID\n",
    "        \n",
    "    Returns:\n",
    "        True if HTML version is available, False otherwise\n",
    "    \"\"\"\n",
    "    html_url = f\"https://arxiv.org/html/{paper_id}\"\n",
    "    response = requests.head(html_url)\n",
    "    return response.status_code == 200\n",
    "\n",
    "\n",
    "def get_html_content(paper_id: str) -> Optional[str]:\n",
    "    \"\"\"Get HTML content of a paper if available.\n",
    "    \n",
    "    Fetches and parses the HTML version of a paper, removing\n",
    "    irrelevant elements and extracting the main content.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: The arXiv paper ID\n",
    "        \n",
    "    Returns:\n",
    "        Extracted text content from the HTML version or None if unavailable\n",
    "    \"\"\"\n",
    "    html_url = f\"https://arxiv.org/html/{paper_id}\"\n",
    "    response = requests.get(html_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Remove scripts, styles, and navigation elements\n",
    "        for tag in soup(['script', 'style', 'nav', 'header', 'footer']):\n",
    "            tag.decompose()\n",
    "        # Get main content\n",
    "        main_content = soup.find('main') or soup.find('body')\n",
    "        if main_content:\n",
    "            return main_content.get_text(separator='\\n', strip=True)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576e276",
   "metadata": {},
   "source": [
    "## Vector Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5877fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_paper_to_vector_db(paper_info: Dict[str, Any]) -> None:\n",
    "    \"\"\"Add a paper to the vector database.\n",
    "    \n",
    "    Stores a paper's metadata and content in the vector database\n",
    "    for semantic search.\n",
    "    \n",
    "    Args:\n",
    "        paper_info: Dictionary containing paper metadata and content\n",
    "    \"\"\"\n",
    "    # Use a combination of title and content for embedding\n",
    "    text_to_embed = f\"{paper_info['title']} {paper_info['content'][:5000]}\"\n",
    "    \n",
    "    # Add to the collection\n",
    "    collection.add(\n",
    "        ids=[paper_info['paper_id']],\n",
    "        documents=[text_to_embed],\n",
    "        metadatas=[{\n",
    "            'title': paper_info['title'],\n",
    "            'authors': paper_info['authors'],\n",
    "            'published': paper_info['published'],\n",
    "            'url': paper_info['url'],\n",
    "            'abstract': paper_info['abstract']\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    print(f\"Added paper to vector DB: {paper_info['title']}\")\n",
    "\n",
    "\n",
    "def query_vector_db(query: str, n_results: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Query the vector database for papers semantically similar to the query.\n",
    "    \n",
    "    Performs a semantic search in the vector database to find\n",
    "    papers most relevant to the query.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        n_results: Maximum number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of paper dictionaries containing metadata\n",
    "    \"\"\"\n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    papers = []\n",
    "    \n",
    "    if results and len(results['ids'][0]) > 0:\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            paper_id = results['ids'][0][i]\n",
    "            metadata = results['metadatas'][0][i]\n",
    "            \n",
    "            # Get the full paper info from our database\n",
    "            if paper_id in papers_db:\n",
    "                papers.append(papers_db[paper_id])\n",
    "            else:\n",
    "                # Reconstruct from metadata if not in cache\n",
    "                papers.append({\n",
    "                    'paper_id': paper_id,\n",
    "                    'title': metadata['title'],\n",
    "                    'authors': metadata['authors'],\n",
    "                    'published': metadata['published'],\n",
    "                    'url': metadata['url'],\n",
    "                    'abstract': metadata['abstract'],\n",
    "                    'content': metadata['abstract']  # Fall back to abstract\n",
    "                })\n",
    "    \n",
    "    return papers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84063ed7",
   "metadata": {},
   "source": [
    "## LangGraph Workflow Nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2b6cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_expansion_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"LangGraph node that expands the original query.\n",
    "    \n",
    "    Takes the original query and generates an expanded version\n",
    "    with additional concepts and search terms.\n",
    "    \n",
    "    Args:\n",
    "        state: The current LangGraph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with expanded query\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    # Use a very explicit prompt to avoid the model repeating our instructions\n",
    "    prompt = f\"\"\"\n",
    "    Please expand the following research query:\n",
    "    \n",
    "    Query: \"{query}\"\n",
    "    \n",
    "    Provide a detailed expansion that identifies key concepts, \n",
    "    terminology, and relevant subtopics. Do not include phrases like\n",
    "    \"Query:\" or \"Expanded:\" in your response. Just provide the expanded content.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    expanded_query = response.text.strip()\n",
    "    \n",
    "    print(\"EXPANSION NODE - Output expanded query:\", expanded_query[:100] + \"...\")\n",
    "    \n",
    "    return {\"expanded_query\": expanded_query}\n",
    "\n",
    "\n",
    "def search_papers_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"LangGraph node that searches for papers based on the query.\n",
    "    \n",
    "    Uses the original and expanded query to search arXiv for relevant\n",
    "    papers, processes them, and stores the results in both the regular\n",
    "    database and the vector database.\n",
    "    \n",
    "    Args:\n",
    "        state: The current LangGraph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with search results\n",
    "    \"\"\"\n",
    "    print(\"SEARCH NODE - Input state keys:\", list(state.keys()))\n",
    "    \n",
    "    query = state[\"query\"]\n",
    "    expanded_query = state[\"expanded_query\"]\n",
    "    \n",
    "    # Extract actual search terms from expanded query\n",
    "    # First, check if expanded query starts with \"Query:\" (which would indicate our formatting issue)\n",
    "    if \"Query:\" in expanded_query and \"Expanded:\" in expanded_query:\n",
    "        # Extract just the expansion part\n",
    "        expanded_query = expanded_query.split(\"Expanded:\")[1].strip()\n",
    "    \n",
    "    # Extract key terms based on domain\n",
    "    domain_specific_terms = []\n",
    "    \n",
    "    # Domain detection logic\n",
    "    if \"vision transformer\" in query.lower() or \"vit\" in query.lower():\n",
    "        domain_specific_terms = [\"Vision Transformer\", \"ViT\", \"image patches\", \n",
    "                          \"self-attention\", \"transformer encoder\", \n",
    "                          \"multi-head attention\", \"computer vision\"]\n",
    "    elif \"graph\" in query.lower() and \"neural\" in query.lower():\n",
    "        domain_specific_terms = [\"Graph Neural Network\", \"GNN\", \"node embedding\",\n",
    "                          \"message passing\", \"graph attention\", \"GraphSAGE\"]\n",
    "    # Add more domain detection as needed\n",
    "    elif \"reinforcement learning\" in query.lower() or \" rl \" in f\" {query.lower()} \":\n",
    "        domain_specific_terms = [\"Reinforcement Learning\", \"RL\", \"policy gradient\", \n",
    "                          \"Q-learning\", \"reward function\", \"MDP\", \"Markov Decision Process\", \n",
    "                          \"DDPG\", \"PPO\", \"TD learning\", \"actor-critic\"]\n",
    "    elif \"large language model\" in query.lower() or \"llm\" in query.lower():\n",
    "        domain_specific_terms = [\"Large Language Model\", \"LLM\", \"transformer\", \n",
    "                          \"attention mechanism\", \"GPT\", \"BERT\", \"prompt engineering\", \n",
    "                          \"fine-tuning\", \"few-shot learning\", \"instruction tuning\"]\n",
    "    elif \"diffusion\" in query.lower() and (\"model\" in query.lower() or \"image\" in query.lower()):\n",
    "        domain_specific_terms = [\"Diffusion Model\", \"DDPM\", \"latent diffusion\", \n",
    "                          \"score-based generative model\", \"noise prediction\", \n",
    "                          \"reverse diffusion\", \"U-Net\", \"text-to-image\"]\n",
    "    elif \"robotics\" in query.lower() or \"robot\" in query.lower():\n",
    "        domain_specific_terms = [\"Robotics\", \"robot learning\", \"manipulation\", \n",
    "                          \"grasping\", \"trajectory optimization\", \"inverse kinematics\", \n",
    "                          \"motion planning\", \"control policy\", \"sim2real\"]\n",
    "    elif \"recommendation\" in query.lower() or \"recommender\" in query.lower():\n",
    "        domain_specific_terms = [\"Recommender System\", \"collaborative filtering\", \n",
    "                          \"content-based filtering\", \"matrix factorization\", \n",
    "                          \"user embedding\", \"item embedding\", \"CTR prediction\"]\n",
    "    elif \"computer vision\" in query.lower() or \"image\" in query.lower():\n",
    "        domain_specific_terms = [\"Computer Vision\", \"CNN\", \"object detection\", \n",
    "                          \"segmentation\", \"image recognition\", \"feature extraction\", \n",
    "                          \"SIFT\", \"ResNet\", \"Faster R-CNN\", \"YOLO\"]\n",
    "    elif (\"natural language\" in query.lower() or \"nlp\" in query.lower()) and \"transformer\" not in query.lower():\n",
    "        domain_specific_terms = [\"Natural Language Processing\", \"NLP\", \"named entity recognition\", \n",
    "                          \"sentiment analysis\", \"text classification\", \"word embedding\", \n",
    "                          \"language model\", \"sequence-to-sequence\", \"LSTM\", \"RNN\"]\n",
    "    elif \"generative\" in query.lower() or \"gan\" in query.lower():\n",
    "        domain_specific_terms = [\"Generative Adversarial Network\", \"GAN\", \"StyleGAN\", \n",
    "                          \"generator\", \"discriminator\", \"adversarial training\", \n",
    "                          \"latent space\", \"mode collapse\", \"image synthesis\"]\n",
    "    elif \"attention\" in query.lower() or \"transformer\" in query.lower():\n",
    "        domain_specific_terms = [\"Transformer\", \"attention mechanism\", \"self-attention\", \n",
    "                          \"multi-head attention\", \"encoder-decoder\", \"positional encoding\", \n",
    "                          \"cross-attention\", \"attention weights\"]\n",
    "    elif \"quantum\" in query.lower() and (\"computing\" in query.lower() or \"machine learning\" in query.lower()):\n",
    "        domain_specific_terms = [\"Quantum Computing\", \"quantum machine learning\", \n",
    "                          \"quantum circuit\", \"qubit\", \"quantum gate\", \"variational quantum circuit\", \n",
    "                          \"QAOA\", \"quantum advantage\", \"quantum supremacy\"]\n",
    "    \n",
    "    # Generic ML terms for any ML-related query\n",
    "    if any(term in query.lower() for term in [\"machine learning\", \"neural network\", \"deep learning\", \"ai\"]):\n",
    "        generic_ml_terms = [\"neural network\", \"deep learning\", \"backpropagation\", \n",
    "                     \"gradient descent\", \"loss function\", \"activation function\", \n",
    "                     \"hyperparameter tuning\", \"regularization\", \"overfitting\"]\n",
    "        domain_specific_terms.extend(generic_ml_terms)\n",
    "    \n",
    "    # If no specific domain is detected, extract key terms from the expanded query\n",
    "    if not domain_specific_terms and expanded_query:\n",
    "        # Extract potential terms from expanded query\n",
    "        expanded_lines = expanded_query.split('. ')\n",
    "        for line in expanded_lines:\n",
    "            # Find capitalized terms or terms in quotes that might be important concepts\n",
    "            import re\n",
    "            potential_terms = re.findall(r'([A-Z][a-zA-Z0-9]+([ \\-][A-Z][a-zA-Z0-9]+)*)', line)\n",
    "            quoted_terms = re.findall(r'\"([^\"]+)\"', line)\n",
    "            \n",
    "            # Add these as domain terms\n",
    "            for term in potential_terms:\n",
    "                if isinstance(term, tuple):\n",
    "                    term = term[0]  # Extract the actual term from regex match tuple\n",
    "                if len(term) > 3 and term not in domain_specific_terms:  # Only terms longer than 3 chars\n",
    "                    domain_specific_terms.append(term)\n",
    "                    \n",
    "            domain_specific_terms.extend(quoted_terms)\n",
    "    \n",
    "    # Log the detected terms\n",
    "    if domain_specific_terms:\n",
    "        print(f\"Detected domain terms: {domain_specific_terms}\")\n",
    "    else:\n",
    "        print(\"No specific domain terms detected, using original query only\")\n",
    "    \n",
    "    # Create a clean search query by combining original and expanded\n",
    "    search_query = query\n",
    "    if domain_specific_terms:\n",
    "        expanded_terms = \" OR \".join(f'\"{term}\"' for term in domain_specific_terms)\n",
    "        search_query = f'\"{query}\" OR ({expanded_terms})'\n",
    "    \n",
    "    print(f\"Clean search query: {search_query}\")\n",
    "    \n",
    "    # Regular search via arXiv API\n",
    "    papers = arxiv_api_search(search_query)\n",
    "    print(f\"Found {len(papers)} papers via API search\")\n",
    "    \n",
    "    # Process and store papers\n",
    "    results = []\n",
    "    for paper in papers:\n",
    "        paper_id = paper.entry_id.split('/')[-1]\n",
    "        \n",
    "        # Skip if already in database\n",
    "        if paper_id in papers_db:\n",
    "            results.append(papers_db[paper_id])\n",
    "            continue\n",
    "            \n",
    "        paper_info = {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"title\": paper.title,\n",
    "            \"authors\": \", \".join(author.name for author in paper.authors),\n",
    "            \"published\": paper.published.strftime(\"%Y-%m-%d\"),\n",
    "            \"url\": paper.entry_id,\n",
    "            \"abstract\": paper.summary,\n",
    "            \"has_html\": check_html_available(paper_id)\n",
    "        }\n",
    "        \n",
    "        # Get HTML content if available\n",
    "        if paper_info[\"has_html\"]:\n",
    "            paper_info[\"content\"] = get_html_content(paper_id)\n",
    "        else:\n",
    "            paper_info[\"content\"] = paper_info[\"abstract\"]\n",
    "            \n",
    "        # Store in our database\n",
    "        papers_db[paper_id] = paper_info\n",
    "        \n",
    "        # Add to vector database\n",
    "        add_paper_to_vector_db(paper_info)\n",
    "        \n",
    "        results.append(paper_info)\n",
    "    \n",
    "    print(f\"Processed {len(results)} papers for state\")\n",
    "    \n",
    "    # Now perform semantic search\n",
    "    semantic_results = query_vector_db(query)\n",
    "    print(f\"Found {len(semantic_results)} papers via semantic search\")\n",
    "    \n",
    "    # Combine results but prioritize semantic search results\n",
    "    combined_results = []\n",
    "    \n",
    "    # First add semantic results\n",
    "    paper_ids_added = set()\n",
    "    for paper in semantic_results:\n",
    "        combined_results.append(paper)\n",
    "        paper_ids_added.add(paper[\"paper_id\"])\n",
    "    \n",
    "    # Then add any API results not already included\n",
    "    for paper in results:\n",
    "        if paper[\"paper_id\"] not in paper_ids_added:\n",
    "            combined_results.append(paper)\n",
    "            paper_ids_added.add(paper[\"paper_id\"])\n",
    "    \n",
    "    # Return updated state\n",
    "    updated_state = {\n",
    "        \"search_results\": combined_results[:10],  # Limit to top 10 papers\n",
    "        \"embedding_results\": semantic_results\n",
    "    }\n",
    "    print(\"SEARCH NODE - Output state keys:\", list(updated_state.keys()))\n",
    "    return updated_state\n",
    "\n",
    "\n",
    "def analyze_papers_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"LangGraph node that analyzes papers.\n",
    "    \n",
    "    Takes the search results and generates an analysis\n",
    "    of the papers in relation to the original query.\n",
    "    \n",
    "    Args:\n",
    "        state: The current LangGraph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with paper analysis and context\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    search_results = state[\"search_results\"]\n",
    "    \n",
    "    # Get the vector search results if available\n",
    "    embedding_results = state.get(\"embedding_results\", [])\n",
    "    \n",
    "    # Prioritize embedding results if available\n",
    "    papers_to_analyze = embedding_results if embedding_results else search_results\n",
    "    \n",
    "    analysis = analyze_papers(query, papers_to_analyze)\n",
    "    \n",
    "    # Extract relevant content for context\n",
    "    context = []\n",
    "    for paper in papers_to_analyze[:5]:\n",
    "        context.append(f\"Title: {paper['title']}\\nAuthors: {paper['authors']}\\nAbstract: {paper['abstract']}\")\n",
    "    \n",
    "    return {\n",
    "        \"analysis\": analysis,\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_response_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"LangGraph node that generates the final response.\n",
    "    \n",
    "    Formats the analysis and adds it to the message history\n",
    "    in the state.\n",
    "    \n",
    "    Args:\n",
    "        state: The current LangGraph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with messages\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    context = state[\"context\"]\n",
    "    analysis = state[\"analysis\"]\n",
    "    \n",
    "    # Format the message as an AI response\n",
    "    message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": analysis\n",
    "    }\n",
    "    \n",
    "    # Add the message to the state\n",
    "    if \"messages\" not in state:\n",
    "        state[\"messages\"] = []\n",
    "    \n",
    "    state[\"messages\"].append(message)\n",
    "    \n",
    "    return {\"messages\": state[\"messages\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ffc61f",
   "metadata": {},
   "source": [
    "## LangGraph Workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bf47f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_research_workflow():\n",
    "    \"\"\"Create a LangGraph workflow for research.\n",
    "    \n",
    "    Defines the workflow graph with nodes for query expansion,\n",
    "    paper search, analysis, and response generation.\n",
    "    \n",
    "    Returns:\n",
    "        A compiled LangGraph workflow\n",
    "    \"\"\"\n",
    "    # Initialize the workflow with the RAGState\n",
    "    workflow = StateGraph(RAGState)\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    workflow.add_node(\"query_expansion\", query_expansion_node)\n",
    "    workflow.add_node(\"search_papers\", search_papers_node)\n",
    "    workflow.add_node(\"analyze_papers\", analyze_papers_node)\n",
    "    workflow.add_node(\"generate_response\", generate_response_node)\n",
    "    \n",
    "    # Add edges to connect the nodes\n",
    "    workflow.add_edge(\"query_expansion\", \"search_papers\")\n",
    "    workflow.add_edge(\"search_papers\", \"analyze_papers\")\n",
    "    workflow.add_edge(\"analyze_papers\", \"generate_response\")\n",
    "    \n",
    "    # Set the entry point\n",
    "    workflow.set_entry_point(\"query_expansion\")\n",
    "    \n",
    "    # Compile the workflow\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c51291",
   "metadata": {},
   "source": [
    "## Interface Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63a5ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_arxiv_langgraph(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Research arXiv papers using the LangGraph workflow with embeddings.\n",
    "    \n",
    "    Main function that executes the full research pipeline\n",
    "    on a given query, using both keyword and semantic search.\n",
    "    \n",
    "    Args:\n",
    "        query: The research query\n",
    "        \n",
    "    Returns:\n",
    "        The final state with all results\n",
    "    \"\"\"\n",
    "    # Create the workflow\n",
    "    workflow = create_research_workflow()\n",
    "    \n",
    "    # Initialize the state\n",
    "    initial_state = {\n",
    "        \"query\": query,\n",
    "        \"expanded_query\": \"\",\n",
    "        \"context\": [],\n",
    "        \"messages\": [],\n",
    "        \"search_results\": [],\n",
    "        \"embedding_results\": [],\n",
    "        \"analysis\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Execute the workflow\n",
    "    final_state = workflow.invoke(initial_state)\n",
    "    \n",
    "    # Debug print state\n",
    "    print(\"Final state keys:\", list(final_state.keys()))\n",
    "    \n",
    "    # Check if search_results exists and has content\n",
    "    if \"search_results\" not in final_state or not final_state[\"search_results\"]:\n",
    "        print(\"WARNING: No search results found in final state!\")\n",
    "        # If the search_results got lost, we should check if it's available in our papers_db\n",
    "        if papers_db:\n",
    "            print(f\"Found {len(papers_db)} papers in papers_db, using those instead\")\n",
    "            final_state[\"search_results\"] = list(papers_db.values())\n",
    "    \n",
    "    return final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6581d80",
   "metadata": {},
   "source": [
    "## Display Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c379da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_langgraph_results(results: Dict[str, Any]) -> None:\n",
    "    \"\"\"Display the research results in a formatted way.\n",
    "    \n",
    "    Creates formatted Markdown outputs for the expanded query,\n",
    "    research analysis, and top papers, with separate sections\n",
    "    for semantic search results.\n",
    "    \n",
    "    Args:\n",
    "        results: The final state from the research workflow\n",
    "    \"\"\"\n",
    "    from IPython.display import display, Markdown, HTML\n",
    "    \n",
    "    display(Markdown(\"### QUERY EXPANSION\"))\n",
    "    display(Markdown(results[\"expanded_query\"]))\n",
    "    \n",
    "    display(Markdown(\"### RESEARCH ANALYSIS\"))\n",
    "    display(Markdown(results[\"analysis\"]))\n",
    "    \n",
    "    # Show vector-based results first\n",
    "    if \"embedding_results\" in results and results[\"embedding_results\"]:\n",
    "        display(Markdown(\"### TOP PAPERS (SEMANTIC SEARCH)\"))\n",
    "        display(Markdown(f\"**Found {len(results['embedding_results'])} papers via semantic search.**\"))\n",
    "        \n",
    "        for i, paper in enumerate(results[\"embedding_results\"][:3]):\n",
    "            paper_md = f\"\"\"\n",
    "                        **{i+1}. {paper['title']}**\n",
    "\n",
    "                        *Authors:* {paper['authors']}\n",
    "\n",
    "                        *Published:* {paper['published']}\n",
    "\n",
    "                        *URL:* {paper['url']}\n",
    "\n",
    "                        *Abstract:* {paper['abstract'][:300]}...\n",
    "\n",
    "                        ---\n",
    "                        \"\"\"\n",
    "            display(Markdown(paper_md))\n",
    "    \n",
    "    # Show all results\n",
    "    display(Markdown(\"### ALL TOP PAPERS\"))\n",
    "    \n",
    "    if \"search_results\" not in results or not results[\"search_results\"]:\n",
    "        display(Markdown(\"**No papers found in search results.**\"))\n",
    "    else:\n",
    "        display(Markdown(f\"**Found {len(results['search_results'])} papers total.**\"))\n",
    "        for i, paper in enumerate(results[\"search_results\"][:5]):\n",
    "            paper_md = f\"\"\"\n",
    "            **{i+1}. {paper['title']}**\n",
    "\n",
    "            *Authors:* {paper['authors']}\n",
    "\n",
    "            *Published:* {paper['published']}\n",
    "\n",
    "            *URL:* {paper['url']}\n",
    "\n",
    "            *Abstract:* {paper['abstract'][:300]}...\n",
    "\n",
    "            ---\n",
    "            \"\"\"\n",
    "            display(Markdown(paper_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e50e0b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPANSION NODE - Output expanded query: Vision Transformers (ViTs) represent a groundbreaking shift in computer vision, applying the Transfo...\n",
      "SEARCH NODE - Input state keys: ['query', 'expanded_query', 'context', 'messages', 'search_results', 'analysis', 'embedding_results']\n",
      "Detected domain terms: ['Vision Transformer', 'ViT', 'image patches', 'self-attention', 'transformer encoder', 'multi-head attention', 'computer vision']\n",
      "Clean search query: \"what are vision transformers?\" OR (\"Vision Transformer\" OR \"ViT\" OR \"image patches\" OR \"self-attention\" OR \"transformer encoder\" OR \"multi-head attention\" OR \"computer vision\")\n",
      "Found 20 papers via API search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2312.03568v1\n",
      "Add of existing embedding ID: 2312.03568v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: DocBinFormer: A Two-Level Transformer Network for Effective Document Image Binarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2207.11971v2\n",
      "Add of existing embedding ID: 2207.11971v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2206.00481v2\n",
      "Add of existing embedding ID: 2206.00481v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: Where are my Neighbors? Exploiting Patches Relations in Self-Supervised Vision Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2205.12041v1\n",
      "Add of existing embedding ID: 2205.12041v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: Privacy-Preserving Image Classification Using Vision Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2108.01684v1\n",
      "Add of existing embedding ID: 2108.01684v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: Vision Transformer with Progressive Sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2406.12944v1\n",
      "Add of existing embedding ID: 2406.12944v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: Semantic Graph Consistency: Going Beyond Patches for Regularizing Self-Supervised Vision Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2203.01587v3\n",
      "Add of existing embedding ID: 2203.01587v3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: Multi-Tailed Vision Transformer for Efficient Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2306.02095v1\n",
      "Add of existing embedding ID: 2306.02095v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2309.08035v1\n",
      "Add of existing embedding ID: 2309.08035v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: Interpretability-Aware Vision Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2501.16227v1\n",
      "Add of existing embedding ID: 2501.16227v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: PDC-ViT : Source Camera Identification using Pixel Difference Convolution and Vision Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2406.18051v1\n",
      "Add of existing embedding ID: 2406.18051v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: ViT-1.58b: Mobile Vision Transformers in the 1-bit Era\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2211.06726v2\n",
      "Add of existing embedding ID: 2211.06726v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: MultiCrossViT: Multimodal Vision Transformer for Schizophrenia Prediction using Structural MRI and Functional Network Connectivity Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2205.14949v1\n",
      "Add of existing embedding ID: 2205.14949v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2205.09995v1\n",
      "Add of existing embedding ID: 2205.09995v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: Mask-guided Vision Transformer (MG-ViT) for Few-Shot Learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2203.08566v1\n",
      "Add of existing embedding ID: 2203.08566v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: EDTER: Edge Detection with Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2204.10485v1\n",
      "Add of existing embedding ID: 2204.10485v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: Attentions Help CNNs See Better: Attention-based Hybrid Image Quality Assessment Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2110.14731v3\n",
      "Add of existing embedding ID: 2110.14731v3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: Vision Transformer for Classification of Breast Ultrasound Images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2203.11987v2\n",
      "Add of existing embedding ID: 2203.11987v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: PaCa-ViT: Learning Patch-to-Cluster Attention in Vision Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2202.01884v1\n",
      "Add of existing embedding ID: 2202.01884v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: Research on Patch Attentive Neural Process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 2407.19394v4\n",
      "Add of existing embedding ID: 2407.19394v4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to get embedding, using random fallback\n",
      "Added paper to vector DB: Depth-Wise Convolutions in Vision Transformers for Efficient Training on Small Datasets\n",
      "Processed 20 papers for state\n",
      "Warning: Failed to get embedding, using random fallback\n",
      "Found 5 papers via semantic search\n",
      "SEARCH NODE - Output state keys: ['search_results', 'embedding_results']\n",
      "Final state keys: ['query', 'expanded_query', 'context', 'messages', 'search_results', 'analysis', 'embedding_results']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### QUERY EXPANSION"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Vision Transformers (ViTs) represent a groundbreaking shift in computer vision, applying the Transformer architecture, originally designed for natural language processing, to image recognition and related tasks.  Understanding ViTs involves exploring several key concepts:\n",
       "\n",
       "* **The Transformer Architecture:**  This foundation relies on self-attention mechanisms, allowing the model to weigh the importance of different parts of an input sequence (in NLP, words; in vision, image patches) in relation to each other.  Key components include:\n",
       "    * **Self-Attention:** The core mechanism enabling the model to relate different parts of the input.  This involves calculating attention weights based on the relationships between different patches, effectively allowing the model to focus on relevant parts of the image.\n",
       "    * **Multi-Head Self-Attention:**  Employing multiple self-attention mechanisms operating in parallel, each focusing on different aspects of the input, leading to a richer representation.\n",
       "    * **Positional Encoding:** Since Transformers inherently lack sequential information, positional encodings are added to the input embeddings to represent the location of each patch within the image.\n",
       "    * **Encoder-Decoder Structure (in some ViTs):** While original ViTs used only the encoder part, some variants incorporate a decoder for tasks like image generation and segmentation.\n",
       "\n",
       "* **Image Patch Embeddings:** ViTs divide images into smaller patches, which are then flattened and linearly projected into embedding vectors.  These embeddings serve as the input to the Transformer encoder.  The patch size is a crucial hyperparameter affecting model performance and computational cost.\n",
       "\n",
       "* **Comparison to Convolutional Neural Networks (CNNs):** Traditionally, CNNs dominated computer vision.  ViTs offer distinct advantages, including:\n",
       "    * **Global Receptive Field:** ViTs can capture long-range dependencies within the image from the initial layers, unlike CNNs which require stacking multiple convolutional layers to achieve a larger receptive field.\n",
       "    * **Scalability:** Transformers scale well with increasing data and model size, leading to improved performance on large datasets.\n",
       "\n",
       "* **Variants and Applications of Vision Transformers:** Beyond the basic ViT architecture, numerous variants have been developed, addressing limitations and extending capabilities:\n",
       "    * **Hybrid Architectures:** Combining CNNs and Transformers to leverage the strengths of both approaches.\n",
       "    * **Hierarchical Transformers:**  Processing images at multiple scales to capture both local and global features more effectively.\n",
       "    * **Vision Transformer based Detection and Segmentation:** Adapting ViTs for object detection, semantic segmentation, and instance segmentation tasks.\n",
       "    * **Self-Supervised Learning with ViTs:**  Leveraging large amounts of unlabeled data to pre-train ViTs for improved performance.\n",
       "\n",
       "* **Challenges and Future Directions:**\n",
       "    * **Computational Cost:** ViTs can be computationally expensive, especially for high-resolution images.  Research focuses on reducing this cost through techniques like efficient attention mechanisms.\n",
       "    * **Data Efficiency:**  While ViTs excel with large datasets, improving their performance on smaller datasets remains a challenge.\n",
       "    * **Interpretability:**  Understanding the internal workings of ViTs and the reasoning behind their predictions is an active area of research.\n",
       "\n",
       "\n",
       "By exploring these concepts and subtopics, one can gain a comprehensive understanding of Vision Transformers, their strengths and limitations, and their potential impact on the future of computer vision."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESEARCH ANALYSIS"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "These papers explore various enhancements and applications of Vision Transformers (ViTs).  \"Multi-Tailed Vision Transformer\" focuses on **efficient inference** by using multiple \"tails\" with shared parameters, aiming to reduce computational costs during deployment. \"Where are my Neighbors?\" tackles the challenge of **training ViTs on smaller datasets** by leveraging patch relationships through self-supervision, mitigating the reliance on massive data.  \"PaCa-ViT\" addresses the **semantic gap between image patches and tokens** by introducing patch-to-cluster attention, potentially improving representation learning. \"Mask-guided Vision Transformer\" applies ViTs to **few-shot learning**, leveraging masks for guidance with limited labeled data. Finally, \"Vision Transformer with Progressive Sampling\" enhances **efficiency during training and inference** by progressively sampling patches, reducing the computational burden of attending to all patches at once. Key themes emerging from these papers include efficiency (both during training and inference), handling smaller datasets, and adapting ViTs to specific tasks like few-shot learning.  They address the limitations of standard ViTs related to computational cost and data requirements.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### TOP PAPERS (SEMANTIC SEARCH)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Found 5 papers via semantic search.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "                        **1. Multi-Tailed Vision Transformer for Efficient Inference**\n",
       "\n",
       "                        *Authors:* Yunke Wang, Bo Du, Wenyuan Wang, Chang Xu\n",
       "\n",
       "                        *Published:* 2022-03-03\n",
       "\n",
       "                        *URL:* http://arxiv.org/abs/2203.01587v3\n",
       "\n",
       "                        *Abstract:* Recently, Vision Transformer (ViT) has achieved promising performance in\n",
       "image recognition and gradually serves as a powerful backbone in various vision\n",
       "tasks. To satisfy the sequential input of Transformer, the tail of ViT first\n",
       "splits each image into a sequence of visual tokens with a fixed length...\n",
       "\n",
       "                        ---\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "                        **2. Where are my Neighbors? Exploiting Patches Relations in Self-Supervised Vision Transformer**\n",
       "\n",
       "                        *Authors:* Guglielmo Camporese, Elena Izzo, Lamberto Ballan\n",
       "\n",
       "                        *Published:* 2022-06-01\n",
       "\n",
       "                        *URL:* http://arxiv.org/abs/2206.00481v2\n",
       "\n",
       "                        *Abstract:* Vision Transformers (ViTs) enabled the use of the transformer architecture on\n",
       "vision tasks showing impressive performances when trained on big datasets.\n",
       "However, on relatively small datasets, ViTs are less accurate given their lack\n",
       "of inductive bias. To this end, we propose a simple but still effect...\n",
       "\n",
       "                        ---\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "                        **3. PaCa-ViT: Learning Patch-to-Cluster Attention in Vision Transformers**\n",
       "\n",
       "                        *Authors:* Ryan Grainger, Thomas Paniagua, Xi Song, Naresh Cuntoor, Mun Wai Lee, Tianfu Wu\n",
       "\n",
       "                        *Published:* 2022-03-22\n",
       "\n",
       "                        *URL:* http://arxiv.org/abs/2203.11987v2\n",
       "\n",
       "                        *Abstract:* Vision Transformers (ViTs) are built on the assumption of treating image\n",
       "patches as ``visual tokens\" and learn patch-to-patch attention. The patch\n",
       "embedding based tokenizer has a semantic gap with respect to its counterpart,\n",
       "the textual tokenizer. The patch-to-patch attention suffers from the quadra...\n",
       "\n",
       "                        ---\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ALL TOP PAPERS"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Found 10 papers total.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "            **1. Multi-Tailed Vision Transformer for Efficient Inference**\n",
       "\n",
       "            *Authors:* Yunke Wang, Bo Du, Wenyuan Wang, Chang Xu\n",
       "\n",
       "            *Published:* 2022-03-03\n",
       "\n",
       "            *URL:* http://arxiv.org/abs/2203.01587v3\n",
       "\n",
       "            *Abstract:* Recently, Vision Transformer (ViT) has achieved promising performance in\n",
       "image recognition and gradually serves as a powerful backbone in various vision\n",
       "tasks. To satisfy the sequential input of Transformer, the tail of ViT first\n",
       "splits each image into a sequence of visual tokens with a fixed length...\n",
       "\n",
       "            ---\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "            **2. Where are my Neighbors? Exploiting Patches Relations in Self-Supervised Vision Transformer**\n",
       "\n",
       "            *Authors:* Guglielmo Camporese, Elena Izzo, Lamberto Ballan\n",
       "\n",
       "            *Published:* 2022-06-01\n",
       "\n",
       "            *URL:* http://arxiv.org/abs/2206.00481v2\n",
       "\n",
       "            *Abstract:* Vision Transformers (ViTs) enabled the use of the transformer architecture on\n",
       "vision tasks showing impressive performances when trained on big datasets.\n",
       "However, on relatively small datasets, ViTs are less accurate given their lack\n",
       "of inductive bias. To this end, we propose a simple but still effect...\n",
       "\n",
       "            ---\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "            **3. PaCa-ViT: Learning Patch-to-Cluster Attention in Vision Transformers**\n",
       "\n",
       "            *Authors:* Ryan Grainger, Thomas Paniagua, Xi Song, Naresh Cuntoor, Mun Wai Lee, Tianfu Wu\n",
       "\n",
       "            *Published:* 2022-03-22\n",
       "\n",
       "            *URL:* http://arxiv.org/abs/2203.11987v2\n",
       "\n",
       "            *Abstract:* Vision Transformers (ViTs) are built on the assumption of treating image\n",
       "patches as ``visual tokens\" and learn patch-to-patch attention. The patch\n",
       "embedding based tokenizer has a semantic gap with respect to its counterpart,\n",
       "the textual tokenizer. The patch-to-patch attention suffers from the quadra...\n",
       "\n",
       "            ---\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "            **4. Mask-guided Vision Transformer (MG-ViT) for Few-Shot Learning**\n",
       "\n",
       "            *Authors:* Yuzhong Chen, Zhenxiang Xiao, Lin Zhao, Lu Zhang, Haixing Dai, David Weizhong Liu, Zihao Wu, Changhe Li, Tuo Zhang, Changying Li, Dajiang Zhu, Tianming Liu, Xi Jiang\n",
       "\n",
       "            *Published:* 2022-05-20\n",
       "\n",
       "            *URL:* http://arxiv.org/abs/2205.09995v1\n",
       "\n",
       "            *Abstract:* Learning with little data is challenging but often inevitable in various\n",
       "application scenarios where the labeled data is limited and costly. Recently,\n",
       "few-shot learning (FSL) gained increasing attention because of its\n",
       "generalizability of prior knowledge to new tasks that contain only a few\n",
       "samples. ...\n",
       "\n",
       "            ---\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "            **5. Vision Transformer with Progressive Sampling**\n",
       "\n",
       "            *Authors:* Xiaoyu Yue, Shuyang Sun, Zhanghui Kuang, Meng Wei, Philip Torr, Wayne Zhang, Dahua Lin\n",
       "\n",
       "            *Published:* 2021-08-03\n",
       "\n",
       "            *URL:* http://arxiv.org/abs/2108.01684v1\n",
       "\n",
       "            *Abstract:* Transformers with powerful global relation modeling abilities have been\n",
       "introduced to fundamental computer vision tasks recently. As a typical example,\n",
       "the Vision Transformer (ViT) directly applies a pure transformer architecture\n",
       "on image classification, by simply splitting images into tokens with a...\n",
       "\n",
       "            ---\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============= Main Execution =============\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter your research query: \")\n",
    "    results = research_arxiv_langgraph(query)\n",
    "    display_langgraph_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d609d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6122b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "what are vision transformers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6207338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
